{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Class Imbalance\n",
    "\n",
    "**Purpose**: Address the severe class imbalance problem discovered in EDA.\n",
    "\n",
    "**Problem Summary** (from EDA):\n",
    "- Forward: 74.2% (7,343 samples) - dominant class\n",
    "- Left: 16.4% (1,620 samples)\n",
    "- Right: 9.5% (937 samples) - severe minority\n",
    "- **Imbalance ratio**: 7.8:1 (Forward:Right)\n",
    "\n",
    "**Why this matters**: \n",
    "- Models will bias toward predicting \"Forward\" to minimize training error\n",
    "- Minority classes (especially Right) will be poorly predicted\n",
    "- Overall accuracy can be high while minority class accuracy is terrible\n",
    "\n",
    "**Goal**: Create balanced training sets using multiple strategies, compare them, save for use in modeling notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# For class imbalance handling\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load temporal splits (created in EDA)\n",
    "# We only balance TRAINING set, validation and test remain unchanged!\n",
    "data = np.load('../data/splits_temporal.npz')\n",
    "\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "print(\"Loaded temporal splits:\")\n",
    "print(f\"Train: {len(X_train)} samples\")\n",
    "print(f\"Val:   {len(X_val)} samples\")\n",
    "print(f\"Test:  {len(X_test)} samples\")\n",
    "\n",
    "# Label names for visualization\n",
    "label_names = {-1: 'Left', 0: 'Forward', 1: 'Right'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review Original Imbalance\n",
    "\n",
    "Let's visualize the class distribution in our training set to see the problem clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count classes in training set\n",
    "train_counts = Counter(y_train)\n",
    "\n",
    "print(\"Training Set Distribution:\")\n",
    "print(\"-\" * 50)\n",
    "for label in [-1, 0, 1]:\n",
    "    count = train_counts[label]\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    print(f\"{label_names[label]:8s}: {count:5d} samples ({percentage:5.1f}%)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate imbalance ratios\n",
    "forward_to_left = train_counts[0] / train_counts[-1]\n",
    "forward_to_right = train_counts[0] / train_counts[1]\n",
    "print(f\"\\nImbalance ratios:\")\n",
    "print(f\"  Forward:Left  = {forward_to_left:.1f}:1\")\n",
    "print(f\"  Forward:Right = {forward_to_right:.1f}:1  ‚Üê Severe!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "labels = ['Left', 'Forward', 'Right']\n",
    "counts = [train_counts[-1], train_counts[0], train_counts[1]]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "bars = ax.bar(labels, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax.set_title('Original Training Set Distribution', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 50,\n",
    "            f'{count}\\n({count/len(y_train)*100:.1f}%)',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Problem: Right class has only 937 samples, while Forward has 4,390!\")\n",
    "print(\"Models trained on this will likely ignore Right turns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Method 1: Class Weights\n",
    "\n",
    "**How it works**: \n",
    "- Don't change the dataset\n",
    "- Instead, change the loss function to penalize mistakes on minority classes more\n",
    "- Formula: `weight[class] = n_samples / (n_classes √ó n_samples_in_class)`\n",
    "\n",
    "**Advantages**:\n",
    "- Simple and fast\n",
    "- No synthetic data created\n",
    "- Works with any model that supports weighted loss\n",
    "\n",
    "**Disadvantages**:\n",
    "- Doesn't increase training data for minority classes\n",
    "- Model still sees fewer minority examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights using sklearn\n",
    "# This computes: total_samples / (n_classes √ó samples_per_class)\n",
    "classes = np.unique(y_train)\n",
    "weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "print(\"Computed Class Weights:\")\n",
    "print(\"-\" * 40)\n",
    "for label, weight in zip(classes, weights_array):\n",
    "    print(f\"{label_names[label]:8s} (label={label:2d}): {weight:.3f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"- Right class has weight {weights_array[np.where(classes == 1)[0][0]]:.2f} (highest)\")\n",
    "print(f\"- Forward class has weight {weights_array[np.where(classes == 0)[0][0]]:.2f} (lowest)\")\n",
    "print(\"‚Üí Mistakes on Right turns will be penalized ~7√ó more than Forward!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weight dictionary for Keras/TensorFlow\n",
    "# Keras expects labels mapped to 0, 1, 2 (not -1, 0, 1)\n",
    "# So we map: -1‚Üí0, 0‚Üí1, 1‚Üí2\n",
    "class_weights_dict = {}\n",
    "for i, label in enumerate([-1, 0, 1]):\n",
    "    class_weights_dict[i] = weights_array[np.where(classes == label)[0][0]]\n",
    "\n",
    "print(\"\\nClass weights dictionary (for Keras):\")\n",
    "print(class_weights_dict)\n",
    "print(\"\\nUsage in Keras:\")\n",
    "print(\"  model.fit(X, y, class_weight=class_weights_dict, ...)\")\n",
    "\n",
    "# Save for use in modeling notebooks\n",
    "np.save('../data/class_weights.npy', class_weights_dict)\n",
    "print(\"\\n‚úÖ Saved to: data/class_weights.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Method 2: SMOTE (Synthetic Minority Over-sampling)\n",
    "\n",
    "**How it works**:\n",
    "- Create synthetic samples for minority classes\n",
    "- For each minority sample, find its k nearest neighbors\n",
    "- Create new samples by interpolating between the sample and its neighbors\n",
    "- Formula: `new_sample = sample + random(0,1) √ó (neighbor - sample)`\n",
    "\n",
    "**Advantages**:\n",
    "- Balances dataset by creating more minority samples\n",
    "- Model sees more varied minority examples\n",
    "- Often improves minority class performance\n",
    "\n",
    "**Disadvantages**:\n",
    "- Creates synthetic (not real) data\n",
    "- Can create unrealistic samples if applied incorrectly\n",
    "- Increases training time (more samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE requires 1D feature vectors, so flatten images\n",
    "# (9900, 64, 64) ‚Üí (9900, 4096)\n",
    "X_train_flat = X_train.reshape(len(X_train), -1)\n",
    "\n",
    "print(f\"Original shape: {X_train.shape}\")\n",
    "print(f\"Flattened shape: {X_train_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE\n",
    "print(\"Applying SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote_flat, y_train_smote = smote.fit_resample(X_train_flat, y_train)\n",
    "\n",
    "# Reshape back to images\n",
    "X_train_smote = X_train_smote_flat.reshape(-1, 64, 64)\n",
    "\n",
    "print(\"\\nBefore SMOTE:\")\n",
    "print(Counter(y_train))\n",
    "print(f\"Total: {len(y_train)} samples\")\n",
    "\n",
    "print(\"\\nAfter SMOTE:\")\n",
    "print(Counter(y_train_smote))\n",
    "print(f\"Total: {len(y_train_smote)} samples\")\n",
    "\n",
    "print(\"\\n‚úÖ Result: All classes now have equal representation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SMOTE-generated samples\n",
    "# Show a few synthetic samples (those added after original dataset)\n",
    "original_size = len(y_train)\n",
    "synthetic_indices = range(original_size, min(original_size + 10, len(y_train_smote)))\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(synthetic_indices):\n",
    "    axes[i].imshow(X_train_smote[idx], cmap='gray')\n",
    "    axes[i].set_title(f\"Synthetic {label_names[y_train_smote[idx]]}\", fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Example SMOTE-Generated Synthetic Samples', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Note: These are interpolations between real samples.\")\n",
    "print(\"They should look realistic (not random noise).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SMOTE-balanced dataset\n",
    "# IMPORTANT: Keep validation and test sets unchanged!\n",
    "np.savez('../data/splits_temporal_smote.npz',\n",
    "         X_train=X_train_smote, y_train=y_train_smote,\n",
    "         X_val=X_val, y_val=y_val,\n",
    "         X_test=X_test, y_test=y_test)\n",
    "\n",
    "print(\"‚úÖ Saved to: data/splits_temporal_smote.npz\")\n",
    "print(f\"   Training samples: {len(X_train_smote)}\")\n",
    "print(f\"   Validation samples: {len(X_val)} (unchanged)\")\n",
    "print(f\"   Test samples: {len(X_test)} (unchanged)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method 3: Random Undersampling\n",
    "\n",
    "**How it works**:\n",
    "- Randomly remove samples from majority class\n",
    "- Keep all minority samples\n",
    "- Result: balanced dataset, but smaller\n",
    "\n",
    "**Advantages**:\n",
    "- Simple and fast\n",
    "- No synthetic data created\n",
    "- Can reduce overfitting on majority class\n",
    "\n",
    "**Disadvantages**:\n",
    "- Throws away real data (bad when data is already limited!)\n",
    "- May lose important majority class patterns\n",
    "- Likely NOT good for our small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply random undersampling\n",
    "print(\"Applying Random Undersampling...\")\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_under_flat, y_train_under = rus.fit_resample(X_train_flat, y_train)\n",
    "\n",
    "# Reshape back\n",
    "X_train_under = X_train_under_flat.reshape(-1, 64, 64)\n",
    "\n",
    "print(\"\\nBefore Undersampling:\")\n",
    "print(Counter(y_train))\n",
    "print(f\"Total: {len(y_train)} samples\")\n",
    "\n",
    "print(\"\\nAfter Undersampling:\")\n",
    "print(Counter(y_train_under))\n",
    "print(f\"Total: {len(y_train_under)} samples\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Warning: We threw away {:.0f}% of our training data!\".format(\n",
    "    (len(y_train) - len(y_train_under)) / len(y_train) * 100\n",
    "))\n",
    "print(\"This is likely NOT a good strategy for our already-small dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save undersampled dataset (for completeness, but probably won't use)\n",
    "np.savez('../data/splits_temporal_undersample.npz',\n",
    "         X_train=X_train_under, y_train=y_train_under,\n",
    "         X_val=X_val, y_val=y_val,\n",
    "         X_test=X_test, y_test=y_test)\n",
    "\n",
    "print(\"‚úÖ Saved to: data/splits_temporal_undersample.npz\")\n",
    "print(\"   (Included for completeness, but not recommended)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Method 4: Combined Approach (SMOTE + Tomek Links)\n",
    "\n",
    "**How it works**:\n",
    "- First, apply SMOTE to oversample minority classes\n",
    "- Then, apply Tomek Links to remove borderline samples\n",
    "- Tomek Links: pairs of opposite-class samples that are nearest neighbors\n",
    "- Removing them cleans the decision boundary\n",
    "\n",
    "**Advantages**:\n",
    "- Balances classes while cleaning noisy samples\n",
    "- Often better than SMOTE alone\n",
    "- Creates cleaner decision boundaries\n",
    "\n",
    "**Disadvantages**:\n",
    "- More complex\n",
    "- Slower than plain SMOTE\n",
    "- May remove useful borderline cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE + Tomek Links\n",
    "print(\"Applying SMOTE + Tomek Links...\")\n",
    "smt = SMOTETomek(random_state=42)\n",
    "X_train_combined_flat, y_train_combined = smt.fit_resample(X_train_flat, y_train)\n",
    "\n",
    "# Reshape back\n",
    "X_train_combined = X_train_combined_flat.reshape(-1, 64, 64)\n",
    "\n",
    "print(\"\\nOriginal:\")\n",
    "print(Counter(y_train))\n",
    "print(f\"Total: {len(y_train)} samples\")\n",
    "\n",
    "print(\"\\nAfter SMOTE + Tomek:\")\n",
    "print(Counter(y_train_combined))\n",
    "print(f\"Total: {len(y_train_combined)} samples\")\n",
    "\n",
    "print(\"\\nüí° Note: Slightly fewer samples than pure SMOTE (Tomek removed borderline cases)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined approach dataset\n",
    "np.savez('../data/splits_temporal_combined.npz',\n",
    "         X_train=X_train_combined, y_train=y_train_combined,\n",
    "         X_val=X_val, y_val=y_val,\n",
    "         X_test=X_test, y_test=y_test)\n",
    "\n",
    "print(\"‚úÖ Saved to: data/splits_temporal_combined.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison of All Methods\n",
    "\n",
    "Let's compare all balancing strategies side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "methods_data = {\n",
    "    'Original': (y_train, 'No balancing'),\n",
    "    'SMOTE': (y_train_smote, 'Oversample minority'),\n",
    "    'Undersample': (y_train_under, 'Undersample majority'),\n",
    "    'SMOTE+Tomek': (y_train_combined, 'Oversample + clean boundary')\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for name, (y, desc) in methods_data.items():\n",
    "    counts = Counter(y)\n",
    "    summary_data.append({\n",
    "        'Method': name,\n",
    "        'Description': desc,\n",
    "        'Total': len(y),\n",
    "        'Left': counts.get(-1, 0),\n",
    "        'Forward': counts.get(0, 0),\n",
    "        'Right': counts.get(1, 0),\n",
    "        'Imbalance Ratio': f\"{counts.get(0, 1) / counts.get(1, 1):.2f}:1\"\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: Comparison of Class Balancing Methods\")\n",
    "print(\"=\"*80)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all methods side-by-side\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, (y, desc)) in enumerate(methods_data.items()):\n",
    "    ax = axes[i]\n",
    "    counts = Counter(y)\n",
    "    \n",
    "    labels = ['Left', 'Forward', 'Right']\n",
    "    values = [counts.get(-1, 0), counts.get(0, 0), counts.get(1, 0)]\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    \n",
    "    bars = ax.bar(labels, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel('Number of Samples', fontsize=11)\n",
    "    ax.set_title(f'{name}\\n({desc})', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels\n",
    "    for bar, count in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + height*0.02,\n",
    "                f'{count}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Comparison of Class Balancing Methods', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recommendations\n",
    "\n",
    "Based on our dataset characteristics and goals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. START WITH: Class Weights\")\n",
    "print(\"   ‚úÖ Simplest approach\")\n",
    "print(\"   ‚úÖ No synthetic data created\")\n",
    "print(\"   ‚úÖ Works well with neural networks\")\n",
    "print(\"   ‚Üí Use: data/class_weights.npy\")\n",
    "\n",
    "print(\"\\n2. IF Class Weights Don't Work: Try SMOTE\")\n",
    "print(\"   ‚úÖ Provides more minority class examples\")\n",
    "print(\"   ‚úÖ Often improves minority class F1 score\")\n",
    "print(\"   ‚ö†Ô∏è  Increases training time (more samples)\")\n",
    "print(\"   ‚Üí Use: data/splits_temporal_smote.npz\")\n",
    "\n",
    "print(\"\\n3. ADVANCED: SMOTE + Tomek\")\n",
    "print(\"   ‚úÖ Cleanest decision boundaries\")\n",
    "print(\"   ‚úÖ May work better than plain SMOTE\")\n",
    "print(\"   ‚ö†Ô∏è  More complex, slower\")\n",
    "print(\"   ‚Üí Use: data/splits_temporal_combined.npz\")\n",
    "\n",
    "print(\"\\n4. AVOID: Random Undersampling\")\n",
    "print(\"   ‚ùå Throws away 79% of training data\")\n",
    "print(\"   ‚ùå Our dataset is already small (5,940 samples)\")\n",
    "print(\"   ‚ùå Likely to underperform\")\n",
    "print(\"   ‚Üí Don't use unless other methods fail\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "print(\"When comparing methods, focus on:\")\n",
    "print(\"  ‚Ä¢ Per-class F1 scores (especially Right class)\")\n",
    "print(\"  ‚Ä¢ Confusion matrix (are Right turns being predicted?)\")\n",
    "print(\"  ‚Ä¢ F1-macro (average of per-class F1, weights all classes equally)\")\n",
    "print(\"  ‚ö†Ô∏è  DON'T rely on overall accuracy alone!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Metadata\n",
    "\n",
    "Save a summary of all methods for easy reference in modeling notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata summary\n",
    "metadata = {\n",
    "    'original_distribution': {\n",
    "        'left': int(train_counts[-1]),\n",
    "        'forward': int(train_counts[0]),\n",
    "        'right': int(train_counts[1]),\n",
    "        'total': int(len(y_train)),\n",
    "        'imbalance_ratio': float(train_counts[0] / train_counts[1])\n",
    "    },\n",
    "    'methods': {\n",
    "        'class_weights': {\n",
    "            'description': 'Inverse frequency weights for loss function',\n",
    "            'file': 'class_weights.npy',\n",
    "            'changes_dataset': False,\n",
    "            'train_size': int(len(y_train)),\n",
    "            'weights': {int(k): float(v) for k, v in class_weights_dict.items()},\n",
    "            'recommendation': 'Start here - simplest and most effective'\n",
    "        },\n",
    "        'smote': {\n",
    "            'description': 'SMOTE oversampling of minority classes',\n",
    "            'file': 'splits_temporal_smote.npz',\n",
    "            'changes_dataset': True,\n",
    "            'train_size': int(len(y_train_smote)),\n",
    "            'distribution': {int(k): int(v) for k, v in Counter(y_train_smote).items()},\n",
    "            'recommendation': 'Try if class weights insufficient for minority class'\n",
    "        },\n",
    "        'undersample': {\n",
    "            'description': 'Random undersampling of majority class',\n",
    "            'file': 'splits_temporal_undersample.npz',\n",
    "            'changes_dataset': True,\n",
    "            'train_size': int(len(y_train_under)),\n",
    "            'distribution': {int(k): int(v) for k, v in Counter(y_train_under).items()},\n",
    "            'recommendation': 'NOT recommended - throws away too much data'\n",
    "        },\n",
    "        'smote_tomek': {\n",
    "            'description': 'SMOTE oversampling + Tomek Links boundary cleaning',\n",
    "            'file': 'splits_temporal_combined.npz',\n",
    "            'changes_dataset': True,\n",
    "            'train_size': int(len(y_train_combined)),\n",
    "            'distribution': {int(k): int(v) for k, v in Counter(y_train_combined).items()},\n",
    "            'recommendation': 'Advanced option - may work better than plain SMOTE'\n",
    "        }\n",
    "    },\n",
    "    'usage_example': {\n",
    "        'class_weights': 'model.fit(X, y, class_weight=np.load(\"class_weights.npy\").item(), ...)',\n",
    "        'smote': 'data = np.load(\"splits_temporal_smote.npz\"); X_train = data[\"X_train\"]',\n",
    "        'combined': 'data = np.load(\"splits_temporal_combined.npz\"); X_train = data[\"X_train\"]'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with open('../data/imbalance_summary.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Metadata saved to: data/imbalance_summary.json\")\n",
    "print(\"\\nYou can read this file in modeling notebooks to see details of each method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "**Files created**:\n",
    "1. `data/class_weights.npy` - Class weights for loss function (**recommended start**)\n",
    "2. `data/splits_temporal_smote.npz` - SMOTE-balanced training set\n",
    "3. `data/splits_temporal_undersample.npz` - Undersampled training set (not recommended)\n",
    "4. `data/splits_temporal_combined.npz` - SMOTE + Tomek Links balanced set\n",
    "5. `data/imbalance_summary.json` - Metadata about all methods\n",
    "\n",
    "**Next steps**:\n",
    "1. In baseline models notebook: Try each method, compare results\n",
    "2. In CNN models notebook: Use best method from baseline experiments\n",
    "3. Focus evaluation on per-class F1 scores, not just accuracy\n",
    "\n",
    "**Expected results**:\n",
    "- Class weights should improve Right class F1 by 10-20%\n",
    "- SMOTE may improve further, but with longer training time\n",
    "- Overall accuracy might go down slightly, but that's okay!\n",
    "- Goal: Balanced performance across all three classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
