{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models\n",
    "\n",
    "**Purpose**: Establish baseline performance using simple models.\n",
    "\n",
    "**Why baselines are important**:\n",
    "- Give us a reference point (\"how good is good?\")\n",
    "- Simple models sometimes outperform complex ones on small datasets\n",
    "- Help us understand if complex models (CNNs) are actually necessary\n",
    "\n",
    "**Models we'll test**:\n",
    "1. **Majority Class**: Always predict \"Forward\" (simplest possible)\n",
    "2. **Random**: Random guessing (worst case)\n",
    "3. **Logistic Regression**: Linear classifier\n",
    "4. **Random Forest**: Non-linear tree-based classifier\n",
    "\n",
    "**For each model**, we'll test three scenarios:\n",
    "- A: Original imbalanced data\n",
    "- B: Original data + class weights\n",
    "- C: TFI-balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# Settings\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "# Label names\n",
    "label_names = {-1: 'Left', 0: 'Forward', 1: 'Right'}\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Temporal Splits (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original temporal splits (from notebook 01)\n",
    "data_original = np.load('../data/splits_temporal.npz')\n",
    "\n",
    "X_train = data_original['X_train']\n",
    "y_train = data_original['y_train']\n",
    "X_val = data_original['X_val']\n",
    "y_val = data_original['y_val']\n",
    "X_test = data_original['X_test']\n",
    "y_test = data_original['y_test']\n",
    "\n",
    "print(\"Original Temporal Splits:\")\n",
    "print(f\"  Train: {len(X_train)} samples - {Counter(y_train)}\")\n",
    "print(f\"  Val:   {len(X_val)} samples\")\n",
    "print(f\"  Test:  {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class weights (from notebook 02)\n",
    "class_weights = np.load('../data/class_weights.npy', allow_pickle=True).item()\n",
    "\n",
    "print(\"Class Weights:\")\n",
    "for k, v in class_weights.items():\n",
    "    label = [-1, 0, 1][k]  # Map 0â†’-1, 1â†’0, 2â†’1\n",
    "    print(f\"  {label_names[label]:8s}: {v:.3f}\")\n",
    "\n",
    "print(\"\\nThese weights will be used in loss functions to penalize minority class errors more.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TFI-Balanced Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFI-balanced splits (from notebook 02)\n",
    "data_tfi = np.load('../data/splits_temporal_tfi.npz')\n",
    "\n",
    "X_train_tfi = data_tfi['X_train']\n",
    "y_train_tfi = data_tfi['y_train']\n",
    "# Val and test are same as original\n",
    "\n",
    "print(\"TFI-Balanced Temporal Splits:\")\n",
    "print(f\"  Train: {len(X_train_tfi)} samples - {Counter(y_train_tfi)}\")\n",
    "print(f\"  Val:   {len(X_val)} samples (same as original)\")\n",
    "print(f\"  Test:  {len(X_test)} samples (same as original)\")\n",
    "\n",
    "print(f\"\\nâœ… Training set increased from {len(X_train)} to {len(X_train_tfi)} samples\")\n",
    "print(\"   (Added interpolated frames for Left and Right classes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Models\n",
    "\n",
    "**Why flatten?** Traditional ML models (not CNNs) expect:\n",
    "- Input: 1D feature vector (not 2D image)\n",
    "- We need to convert 64Ã—64 image â†’ 4,096 features\n",
    "\n",
    "**Normalization**: Scale pixel values from [0, 255] to [0, 1]\n",
    "- Helps models converge faster\n",
    "- Prevents features with larger values from dominating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images to 1D vectors\n",
    "# Original: (samples, 64, 64) â†’ (samples, 4096)\n",
    "X_train_flat = X_train.reshape(len(X_train), -1)\n",
    "X_val_flat = X_val.reshape(len(X_val), -1)\n",
    "X_test_flat = X_test.reshape(len(X_test), -1)\n",
    "\n",
    "# TFI-balanced\n",
    "X_train_tfi_flat = X_train_tfi.reshape(len(X_train_tfi), -1)\n",
    "\n",
    "print(\"Flattened shapes:\")\n",
    "print(f\"  X_train: {X_train.shape} â†’ {X_train_flat.shape}\")\n",
    "print(f\"  X_train_tfi: {X_train_tfi.shape} â†’ {X_train_tfi_flat.shape}\")\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "X_train_flat = X_train_flat / 255.0\n",
    "X_val_flat = X_val_flat / 255.0\n",
    "X_test_flat = X_test_flat / 255.0\n",
    "X_train_tfi_flat = X_train_tfi_flat / 255.0\n",
    "\n",
    "print(\"\\nâœ… Data normalized to [0, 1] range\")\n",
    "print(f\"   Min value: {X_train_flat.min():.3f}, Max value: {X_train_flat.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions for Evaluation\n",
    "\n",
    "**Purpose**: Create reusable functions to evaluate all models consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of model predictions.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        model_name: Name for printing\n",
    "    \n",
    "    Returns:\n",
    "        dict with all metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')  # Average of per-class F1\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None, labels=[-1, 0, 1])\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[-1, 0, 1])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Overall Accuracy: {acc:.3f} ({acc*100:.1f}%)\")\n",
    "    print(f\"F1-Macro (avg):   {f1_macro:.3f}\")\n",
    "    print(\"\\nPer-class F1 scores:\")\n",
    "    for label, f1 in zip([-1, 0, 1], f1_per_class):\n",
    "        print(f\"  {label_names[label]:8s}: {f1:.3f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                                target_names=['Left', 'Forward', 'Right'],\n",
    "                                labels=[-1, 0, 1]))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_left': f1_per_class[0],\n",
    "        'f1_forward': f1_per_class[1],\n",
    "        'f1_right': f1_per_class[2],\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as heatmap.\n",
    "    \n",
    "    Args:\n",
    "        cm: Confusion matrix from sklearn\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Left', 'Forward', 'Right'],\n",
    "                yticklabels=['Left', 'Forward', 'Right'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    print(\"Per-class accuracy (diagonal / row sum):\")\n",
    "    for i, label in enumerate([-1, 0, 1]):\n",
    "        class_acc = cm[i, i] / cm[i, :].sum()\n",
    "        print(f\"  {label_names[label]:8s}: {class_acc:.3f} ({class_acc*100:.1f}%)\")\n",
    "\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline 1: Majority Class Classifier\n",
    "\n",
    "**What it does**: Always predicts \"Forward\" (the most common class)\n",
    "\n",
    "**Why test this?**\n",
    "- Simplest possible strategy\n",
    "- Shows the \"do nothing\" baseline\n",
    "- Due to 74% Forward samples, this will get 74% accuracy!\n",
    "- BUT: 0% accuracy on turns (completely useless for actual driving)\n",
    "\n",
    "**Key insight**: This shows why overall accuracy is misleading for imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create majority class classifier\n",
    "# DummyClassifier with strategy='most_frequent' always predicts most common class\n",
    "majority_clf = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "majority_clf.fit(X_train_flat, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_majority = majority_clf.predict(X_test_flat)\n",
    "\n",
    "# Evaluate\n",
    "results_majority = evaluate_model(y_test, y_pred_majority, \"Majority Class Baseline\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(results_majority['confusion_matrix'], \n",
    "                      \"Majority Class Baseline - Confusion Matrix\")\n",
    "\n",
    "print(\"\\nðŸ’¡ OBSERVATION:\")\n",
    "print(\"High overall accuracy (74%) but ZERO predictions for Left or Right!\")\n",
    "print(\"This model is useless for actual driving - it never turns.\")\n",
    "print(\"â†’ This is why we need to look at per-class F1 scores, not just accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline 2: Random Classifier\n",
    "\n",
    "**What it does**: Randomly predicts Left/Forward/Right with equal probability\n",
    "\n",
    "**Why test this?**\n",
    "- Absolute worst-case baseline\n",
    "- Expected accuracy: ~33% (random chance for 3 classes)\n",
    "- Any real model should beat this easily\n",
    "\n",
    "**If a model performs worse than random**: Something is seriously wrong with the implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random classifier\n",
    "# strategy='uniform' predicts each class with equal probability\n",
    "random_clf = DummyClassifier(strategy='uniform', random_state=42)\n",
    "random_clf.fit(X_train_flat, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_random = random_clf.predict(X_test_flat)\n",
    "\n",
    "# Evaluate\n",
    "results_random = evaluate_model(y_test, y_pred_random, \"Random Baseline\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(results_random['confusion_matrix'],\n",
    "                      \"Random Baseline - Confusion Matrix\")\n",
    "\n",
    "print(\"\\nðŸ’¡ OBSERVATION:\")\n",
    "print(f\"Random guessing achieves ~{results_random['accuracy']*100:.1f}% accuracy\")\n",
    "print(\"Any model below this is broken. Any model should beat this easily.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 1: Logistic Regression\n",
    "\n",
    "**What is Logistic Regression?**\n",
    "- Linear classifier: finds a straight line (actually hyperplane) to separate classes\n",
    "- Learns: `prediction = sign(wâ‚Ã—pixelâ‚ + wâ‚‚Ã—pixelâ‚‚ + ... + wâ‚„â‚€â‚‰â‚†Ã—pixelâ‚„â‚€â‚‰â‚† + bias)`\n",
    "- Simple, fast, interpretable\n",
    "\n",
    "**Expected performance** (based on EDA findings):\n",
    "- PCA showed classes are NOT linearly separable\n",
    "- Classes overlap completely in 2D projection\n",
    "- **Prediction: 55-60% accuracy** (struggling with non-linear patterns)\n",
    "\n",
    "**We'll test three scenarios**:\n",
    "- A: Original imbalanced data\n",
    "- B: Original data + class weights\n",
    "- C: TFI-balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5A: Logistic Regression - Original Data (No Balancing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression on original imbalanced data\n",
    "print(\"Training Logistic Regression on original data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_original = LogisticRegression(\n",
    "    max_iter=1000,           # Maximum iterations for convergence\n",
    "    random_state=42,\n",
    "    multi_class='multinomial',  # Use softmax for 3 classes\n",
    "    solver='lbfgs'           # Optimization algorithm\n",
    ")\n",
    "\n",
    "lr_original.fit(X_train_flat, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_lr_original = lr_original.predict(X_test_flat)\n",
    "\n",
    "# Evaluate\n",
    "results_lr_original = evaluate_model(y_test, y_pred_lr_original, \n",
    "                                     \"Logistic Regression (No Balancing)\")\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(results_lr_original['confusion_matrix'],\n",
    "                      \"Logistic Regression (Original) - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5B: Logistic Regression - Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression with class weights\n",
    "# sklearn's class_weight='balanced' automatically computes weights\n",
    "print(\"Training Logistic Regression with class weights...\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_weighted = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced'  # Use balanced class weights\n",
    ")\n",
    "\n",
    "lr_weighted.fit(X_train_flat, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predict\n",
    "y_pred_lr_weighted = lr_weighted.predict(X_test_flat)\n",
    "\n",
    "# Evaluate\n",
    "results_lr_weighted = evaluate_model(y_test, y_pred_lr_weighted,\n",
    "                                     \"Logistic Regression (Class Weights)\")\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "\n",
    "# Plot\n",
    "plot_confusion_matrix(results_lr_weighted['confusion_matrix'],\n",
    "                      \"Logistic Regression (Weighted) - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5C: Logistic Regression - TFI Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression on TFI-balanced data\n",
    "print(\"Training Logistic Regression on TFI-balanced data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_tfi = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "lr_tfi.fit(X_train_tfi_flat, y_train_tfi)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predict\n",
    "y_pred_lr_tfi = lr_tfi.predict(X_test_flat)\n",
    "\n",
    "# Evaluate\n",
    "results_lr_tfi = evaluate_model(y_test, y_pred_lr_tfi,\n",
    "                                \"Logistic Regression (TFI Balanced)\")\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "print(f\"Training set size: {len(X_train_tfi)} (vs {len(X_train)} original)\")\n",
    "\n",
    "# Plot\n",
    "plot_confusion_matrix(results_lr_tfi['confusion_matrix'],\n",
    "                      \"Logistic Regression (TFI) - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Logistic Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "lr_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'No Balancing',\n",
    "        'Accuracy': f\"{results_lr_original['accuracy']:.3f}\",\n",
    "        'F1-Macro': f\"{results_lr_original['f1_macro']:.3f}\",\n",
    "        'F1-Left': f\"{results_lr_original['f1_left']:.3f}\",\n",
    "        'F1-Forward': f\"{results_lr_original['f1_forward']:.3f}\",\n",
    "        'F1-Right': f\"{results_lr_original['f1_right']:.3f}\"\n",
    "    },\n",
    "    {\n",
    "        'Method': 'Class Weights',\n",
    "        'Accuracy': f\"{results_lr_weighted['accuracy']:.3f}\",\n",
    "        'F1-Macro': f\"{results_lr_weighted['f1_macro']:.3f}\",\n",
    "        'F1-Left': f\"{results_lr_weighted['f1_left']:.3f}\",\n",
    "        'F1-Forward': f\"{results_lr_weighted['f1_forward']:.3f}\",\n",
    "        'F1-Right': f\"{results_lr_weighted['f1_right']:.3f}\"\n",
    "    },\n",
    "    {\n",
    "        'Method': 'TFI Balanced',\n",
    "        'Accuracy': f\"{results_lr_tfi['accuracy']:.3f}\",\n",
    "        'F1-Macro': f\"{results_lr_tfi['f1_macro']:.3f}\",\n",
    "        'F1-Left': f\"{results_lr_tfi['f1_left']:.3f}\",\n",
    "        'F1-Forward': f\"{results_lr_tfi['f1_forward']:.3f}\",\n",
    "        'F1-Right': f\"{results_lr_tfi['f1_right']:.3f}\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOGISTIC REGRESSION: Comparison of Balancing Methods\")\n",
    "print(\"=\"*80)\n",
    "print(lr_comparison.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ’¡ KEY OBSERVATIONS:\")\n",
    "print(\"- Overall accuracy may DECREASE with balancing (that's okay!)\")\n",
    "print(\"- F1 scores for minority classes should INCREASE (that's what we want!)\")\n",
    "print(\"- F1-Macro gives equal weight to all classes (better metric than accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 2: Random Forest\n",
    "\n",
    "**What is Random Forest?**\n",
    "- Ensemble of decision trees (collection of many trees voting)\n",
    "- Each tree learns if-then rules: \"if pixel[100] > 120 AND pixel[200] < 50, predict Left\"\n",
    "- Non-linear: can learn complex patterns\n",
    "- Less prone to overfitting than single decision tree\n",
    "\n",
    "**Expected performance** (based on EDA):\n",
    "- Can learn some spatial patterns (better than Logistic Regression)\n",
    "- **Prediction: 60-65% accuracy**\n",
    "- Should handle non-linearity better (PCA showed classes overlap in linear space)\n",
    "\n",
    "**Hyperparameters**:\n",
    "- `n_estimators`: Number of trees (more = better, but slower)\n",
    "- `max_depth`: Maximum tree depth (prevent overfitting)\n",
    "- `min_samples_split`: Minimum samples to split a node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6A: Random Forest - Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest on original data\n",
    "print(\"Training Random Forest on original data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_original = RandomForestClassifier(\n",
    "    n_estimators=100,        # 100 trees\n",
    "    max_depth=20,            # Limit depth to prevent overfitting\n",
    "    min_samples_split=10,    # Need at least 10 samples to split\n",
    "    random_state=42,\n",
    "    n_jobs=-1                # Use all CPU cores\n",
    ")\n",
    "\n",
    "rf_original.fit(X_train_flat, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predict\n",
    "y_pred_rf_original = rf_original.predict(X_test_flat)\n",
    "\n",
    "# Evaluate\n",
    "results_rf_original = evaluate_model(y_test, y_pred_rf_original,\n",
    "                                     \"Random Forest (No Balancing)\")\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "\n",
    "# Plot\n",
    "plot_confusion_matrix(results_rf_original['confusion_matrix'],\n",
    "                      \"Random Forest (Original) - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6B: Random Forest - Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest with balanced class weights\n",
    "print(\"Training Random Forest with class weights...\")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_weighted = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'  # Use balanced weights\n",
    ")\n",
    "\n",
    "rf_weighted.fit(X_train_flat, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predict\n",
    "y_pred_rf_weighted = rf_weighted.predict(X_test_flat)\n",
    "\n",
    "# Evaluate\n",
    "results_rf_weighted = evaluate_model(y_test, y_pred_rf_weighted,\n",
    "                                     \"Random Forest (Class Weights)\")\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "\n",
    "# Plot\n",
    "plot_confusion_matrix(results_rf_weighted['confusion_matrix'],\n",
    "                      \"Random Forest (Weighted) - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6C: Random Forest - TFI Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest on TFI-balanced data\n",
    "print(\"Training Random Forest on TFI-balanced data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_tfi = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_tfi.fit(X_train_tfi_flat, y_train_tfi)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predict\n",
    "y_pred_rf_tfi = rf_tfi.predict(X_test_flat)\n",
    "\n",
    "# Evaluate\n",
    "results_rf_tfi = evaluate_model(y_test, y_pred_rf_tfi,\n",
    "                                \"Random Forest (TFI Balanced)\")\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")\n",
    "print(f\"Training set size: {len(X_train_tfi)} (vs {len(X_train)} original)\")\n",
    "\n",
    "# Plot\n",
    "plot_confusion_matrix(results_rf_tfi['confusion_matrix'],\n",
    "                      \"Random Forest (TFI) - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Random Forest Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "rf_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'No Balancing',\n",
    "        'Accuracy': f\"{results_rf_original['accuracy']:.3f}\",\n",
    "        'F1-Macro': f\"{results_rf_original['f1_macro']:.3f}\",\n",
    "        'F1-Left': f\"{results_rf_original['f1_left']:.3f}\",\n",
    "        'F1-Forward': f\"{results_rf_original['f1_forward']:.3f}\",\n",
    "        'F1-Right': f\"{results_rf_original['f1_right']:.3f}\"\n",
    "    },\n",
    "    {\n",
    "        'Method': 'Class Weights',\n",
    "        'Accuracy': f\"{results_rf_weighted['accuracy']:.3f}\",\n",
    "        'F1-Macro': f\"{results_rf_weighted['f1_macro']:.3f}\",\n",
    "        'F1-Left': f\"{results_rf_weighted['f1_left']:.3f}\",\n",
    "        'F1-Forward': f\"{results_rf_weighted['f1_forward']:.3f}\",\n",
    "        'F1-Right': f\"{results_rf_weighted['f1_right']:.3f}\"\n",
    "    },\n",
    "    {\n",
    "        'Method': 'TFI Balanced',\n",
    "        'Accuracy': f\"{results_rf_tfi['accuracy']:.3f}\",\n",
    "        'F1-Macro': f\"{results_rf_tfi['f1_macro']:.3f}\",\n",
    "        'F1-Left': f\"{results_rf_tfi['f1_left']:.3f}\",\n",
    "        'F1-Forward': f\"{results_rf_tfi['f1_forward']:.3f}\",\n",
    "        'F1-Right': f\"{results_rf_tfi['f1_right']:.3f}\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANDOM FOREST: Comparison of Balancing Methods\")\n",
    "print(\"=\"*80)\n",
    "print(rf_comparison.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Overall Comparison\n",
    "\n",
    "Compare all baseline models to identify the best performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "all_results = pd.DataFrame([\n",
    "    {'Model': 'Majority Class', 'Balancing': 'N/A', \n",
    "     'Accuracy': f\"{results_majority['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_majority['f1_macro']:.3f}\",\n",
    "     'F1-Left': f\"{results_majority['f1_left']:.3f}\",\n",
    "     'F1-Right': f\"{results_majority['f1_right']:.3f}\"},\n",
    "    \n",
    "    {'Model': 'Random', 'Balancing': 'N/A',\n",
    "     'Accuracy': f\"{results_random['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_random['f1_macro']:.3f}\",\n",
    "     'F1-Left': f\"{results_random['f1_left']:.3f}\",\n",
    "     'F1-Right': f\"{results_random['f1_right']:.3f}\"},\n",
    "    \n",
    "    {'Model': 'Logistic Regression', 'Balancing': 'None',\n",
    "     'Accuracy': f\"{results_lr_original['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_lr_original['f1_macro']:.3f}\",\n",
    "     'F1-Left': f\"{results_lr_original['f1_left']:.3f}\",\n",
    "     'F1-Right': f\"{results_lr_original['f1_right']:.3f}\"},\n",
    "    \n",
    "    {'Model': 'Logistic Regression', 'Balancing': 'Class Weights',\n",
    "     'Accuracy': f\"{results_lr_weighted['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_lr_weighted['f1_macro']:.3f}\",\n",
    "     'F1-Left': f\"{results_lr_weighted['f1_left']:.3f}\",\n",
    "     'F1-Right': f\"{results_lr_weighted['f1_right']:.3f}\"},\n",
    "    \n",
    "    {'Model': 'Logistic Regression', 'Balancing': 'TFI',\n",
    "     'Accuracy': f\"{results_lr_tfi['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_lr_tfi['f1_macro']:.3f}\",\n",
    "     'F1-Left': f\"{results_lr_tfi['f1_left']:.3f}\",\n",
    "     'F1-Right': f\"{results_lr_tfi['f1_right']:.3f}\"},\n",
    "    \n",
    "    {'Model': 'Random Forest', 'Balancing': 'None',\n",
    "     'Accuracy': f\"{results_rf_original['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_rf_original['f1_macro']:.3f}\",\n",
    "     'F1-Left': f\"{results_rf_original['f1_left']:.3f}\",\n",
    "     'F1-Right': f\"{results_rf_original['f1_right']:.3f}\"},\n",
    "    \n",
    "    {'Model': 'Random Forest', 'Balancing': 'Class Weights',\n",
    "     'Accuracy': f\"{results_rf_weighted['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_rf_weighted['f1_macro']:.3f}\",\n",
    "     'F1-Left': f\"{results_rf_weighted['f1_left']:.3f}\",\n",
    "     'F1-Right': f\"{results_rf_weighted['f1_right']:.3f}\"},\n",
    "    \n",
    "    {'Model': 'Random Forest', 'Balancing': 'TFI',\n",
    "     'Accuracy': f\"{results_rf_tfi['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_rf_tfi['f1_macro']:.3f}\",\n",
    "     'F1-Left': f\"{results_rf_tfi['f1_left']:.3f}\",\n",
    "     'F1-Right': f\"{results_rf_tfi['f1_right']:.3f}\"}\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"ALL BASELINE MODELS: Complete Comparison\")\n",
    "print(\"=\"*90)\n",
    "print(all_results.to_string(index=False))\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize F1 scores across models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: F1-Macro comparison\n",
    "models = ['Majority', 'Random', 'LR-Orig', 'LR-Weight', 'LR-TFI', 'RF-Orig', 'RF-Weight', 'RF-TFI']\n",
    "f1_macros = [\n",
    "    results_majority['f1_macro'],\n",
    "    results_random['f1_macro'],\n",
    "    results_lr_original['f1_macro'],\n",
    "    results_lr_weighted['f1_macro'],\n",
    "    results_lr_tfi['f1_macro'],\n",
    "    results_rf_original['f1_macro'],\n",
    "    results_rf_weighted['f1_macro'],\n",
    "    results_rf_tfi['f1_macro']\n",
    "]\n",
    "\n",
    "axes[0].bar(range(len(models)), f1_macros, color='steelblue', alpha=0.7)\n",
    "axes[0].set_xticks(range(len(models)))\n",
    "axes[0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('F1-Macro Score', fontsize=12)\n",
    "axes[0].set_title('F1-Macro Comparison (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Plot 2: Per-class F1 for best models\n",
    "# Compare best LR vs best RF\n",
    "best_models = ['LR-Weight', 'LR-TFI', 'RF-Weight', 'RF-TFI']\n",
    "best_results = [results_lr_weighted, results_lr_tfi, results_rf_weighted, results_rf_tfi]\n",
    "\n",
    "x = np.arange(len(best_models))\n",
    "width = 0.25\n",
    "\n",
    "f1_left = [r['f1_left'] for r in best_results]\n",
    "f1_forward = [r['f1_forward'] for r in best_results]\n",
    "f1_right = [r['f1_right'] for r in best_results]\n",
    "\n",
    "axes[1].bar(x - width, f1_left, width, label='Left', color='#FF6B6B', alpha=0.8)\n",
    "axes[1].bar(x, f1_forward, width, label='Forward', color='#4ECDC4', alpha=0.8)\n",
    "axes[1].bar(x + width, f1_right, width, label='Right', color='#45B7D1', alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "axes[1].set_title('Per-Class F1 Scores', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(best_models)\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis (Random Forest)\n",
    "\n",
    "**What is feature importance?**\n",
    "- Random Forest can tell us which pixels are most useful for predictions\n",
    "- Higher importance = pixel is more useful for classification\n",
    "\n",
    "**Why this is interesting**:\n",
    "- Shows which parts of the image the model looks at\n",
    "- Can visualize as a 64Ã—64 heatmap\n",
    "- Validates if model is learning sensible patterns (e.g., bottom edges for steering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from best Random Forest model\n",
    "# Use the TFI-balanced model as example\n",
    "importances = rf_tfi.feature_importances_\n",
    "\n",
    "# Reshape to image format (64Ã—64)\n",
    "importance_map = importances.reshape(64, 64)\n",
    "\n",
    "# Plot as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(importance_map, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar(label='Feature Importance')\n",
    "plt.title('Random Forest Feature Importance Map\\n(Which pixels matter most?)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('X coordinate', fontsize=12)\n",
    "plt.ylabel('Y coordinate', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ INTERPRETATION:\")\n",
    "print(\"Brighter areas = more important pixels for classification\")\n",
    "print(\"Expected: Bottom edges should be bright (used for steering decisions)\")\n",
    "print(\"If center is bright: Model learned to look at track position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Findings\n",
    "\n",
    "**Baseline models tested**:\n",
    "1. Majority class baseline\n",
    "2. Random baseline\n",
    "3. Logistic Regression (linear)\n",
    "4. Random Forest (non-linear)\n",
    "\n",
    "**Class balancing methods compared**:\n",
    "- None (original imbalanced data)\n",
    "- Class weights\n",
    "- TFI (Temporal Frame Interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"SUMMARY: Key Findings from Baseline Models\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Find best overall model\n",
    "best_f1_macro = max([\n",
    "    results_lr_original['f1_macro'],\n",
    "    results_lr_weighted['f1_macro'],\n",
    "    results_lr_tfi['f1_macro'],\n",
    "    results_rf_original['f1_macro'],\n",
    "    results_rf_weighted['f1_macro'],\n",
    "    results_rf_tfi['f1_macro']\n",
    "])\n",
    "\n",
    "print(\"\\n1. BASELINE PERFORMANCE BOUNDS:\")\n",
    "print(f\"   â€¢ Worst case (Random):        {results_random['accuracy']:.1%} accuracy\")\n",
    "print(f\"   â€¢ Naive baseline (Majority):  {results_majority['accuracy']:.1%} accuracy (but 0% on turns!)\")\n",
    "print(f\"   â€¢ Best baseline F1-Macro:     {best_f1_macro:.3f}\")\n",
    "\n",
    "print(\"\\n2. LOGISTIC REGRESSION FINDINGS:\")\n",
    "print(f\"   â€¢ Original data:     {results_lr_original['accuracy']:.1%} accuracy\")\n",
    "print(f\"   â€¢ With class weights: {results_lr_weighted['accuracy']:.1%} accuracy\")\n",
    "print(f\"   â€¢ With TFI:          {results_lr_tfi['accuracy']:.1%} accuracy\")\n",
    "print(\"   â†’ Confirms EDA prediction: Linear model struggles (PCA showed overlap)\")\n",
    "print(\"   â†’ Class balancing helps minority classes but overall accuracy modest\")\n",
    "\n",
    "print(\"\\n3. RANDOM FOREST FINDINGS:\")\n",
    "print(f\"   â€¢ Original data:     {results_rf_original['accuracy']:.1%} accuracy\")\n",
    "print(f\"   â€¢ With class weights: {results_rf_weighted['accuracy']:.1%} accuracy\")\n",
    "print(f\"   â€¢ With TFI:          {results_rf_tfi['accuracy']:.1%} accuracy\")\n",
    "print(\"   â†’ Non-linear model outperforms linear (as expected from PCA)\")\n",
    "print(\"   â†’ Shows spatial patterns exist, but linear methods can't capture them\")\n",
    "\n",
    "print(\"\\n4. CLASS BALANCING IMPACT:\")\n",
    "lr_right_improvement = results_lr_weighted['f1_right'] - results_lr_original['f1_right']\n",
    "rf_right_improvement = results_rf_weighted['f1_right'] - results_rf_original['f1_right']\n",
    "print(f\"   â€¢ LR Right-class F1 improved by: {lr_right_improvement:.3f} with class weights\")\n",
    "print(f\"   â€¢ RF Right-class F1 improved by: {rf_right_improvement:.3f} with class weights\")\n",
    "print(\"   âœ… Class balancing successfully improves minority class performance!\")\n",
    "\n",
    "print(\"\\n5. WHAT THIS TELLS US ABOUT CNNs:\")\n",
    "print(f\"   â€¢ Best baseline F1-Macro: {best_f1_macro:.3f}\")\n",
    "print(\"   â€¢ CNNs should beat this by learning spatial hierarchies\")\n",
    "print(\"   â€¢ If CNN < 0.65 F1-Macro: something is wrong (overfitting, bad architecture)\")\n",
    "print(\"   â€¢ If CNN > 0.75 F1-Macro: spatial features are being learned successfully\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"RECOMMENDATIONS FOR NEXT STEPS\")\n",
    "print(\"=\"*90)\n",
    "print(\"For CNN models (Notebook 04):\")\n",
    "print(\"  1. Use class weights OR TFI (whichever worked better here)\")\n",
    "print(\"  2. Target: F1-Macro > {:.2f} (beat best baseline)\".format(best_f1_macro))\n",
    "print(\"  3. Focus on Right class F1 (hardest to predict)\")\n",
    "print(\"  4. Use temporal splits only (no random split for final models)\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results\n",
    "\n",
    "Save results for comparison with CNN models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to file\n",
    "baseline_results = {\n",
    "    'majority': results_majority,\n",
    "    'random': results_random,\n",
    "    'lr_original': results_lr_original,\n",
    "    'lr_weighted': results_lr_weighted,\n",
    "    'lr_tfi': results_lr_tfi,\n",
    "    'rf_original': results_rf_original,\n",
    "    'rf_weighted': results_rf_weighted,\n",
    "    'rf_tfi': results_rf_tfi\n",
    "}\n",
    "\n",
    "# Convert numpy arrays to lists for JSON serialization\n",
    "for key in baseline_results:\n",
    "    if 'confusion_matrix' in baseline_results[key]:\n",
    "        baseline_results[key]['confusion_matrix'] = baseline_results[key]['confusion_matrix'].tolist()\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "with open('../results/baseline_results.json', 'w') as f:\n",
    "    json.dump(baseline_results, f, indent=2)\n",
    "\n",
    "print(\"âœ… Results saved to: results/baseline_results.json\")\n",
    "print(\"\\nThese results will be used for comparison in later notebooks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Notebook 04: CNN Models**\n",
    "- Test convolutional neural networks\n",
    "- Leverage spatial structure (what linear models missed)\n",
    "- Expected: 65-75% accuracy, F1-Macro > 0.65\n",
    "- Compare simple CNN vs deeper architectures\n",
    "\n",
    "**Notebook 05: Temporal Models**  \n",
    "- LSTM on frame sequences\n",
    "- Address temporal lag problem\n",
    "- Expected: Best performance (70-80% accuracy)\n",
    "\n",
    "**Goal**: Beat the baseline F1-Macro of {:.3f}!\".format(best_f1_macro)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
