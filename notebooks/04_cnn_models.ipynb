{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) Models\n",
    "\n",
    "**Goal**: Test if CNNs can beat the 93.0% / 0.910 F1-Macro achieved by Random Forest.\n",
    "\n",
    "## What is a CNN?\n",
    "\n",
    "**Convolutional Neural Network** = specialized neural network for images\n",
    "\n",
    "**Key difference from traditional models**:\n",
    "- Traditional (LR, RF): Treat image as flat list of 4,096 pixels (loses spatial structure)\n",
    "- CNN: Preserves 2D structure, learns spatial patterns\n",
    "\n",
    "**How CNNs work**:\n",
    "1. **Convolutional layers**: Scan small filters (e.g., 3×3) across image to detect patterns\n",
    "   - Early layers: detect edges, corners\n",
    "   - Middle layers: detect shapes, textures\n",
    "   - Deep layers: detect complex patterns (track positions)\n",
    "\n",
    "2. **Pooling layers**: Reduce image size while keeping important features\n",
    "   - Makes model faster and more robust\n",
    "\n",
    "3. **Dense layers**: Combine all features to make final prediction\n",
    "\n",
    "**Example architecture**:\n",
    "```\n",
    "Input 64×64 image\n",
    "→ Conv (find edges)\n",
    "→ Pool (reduce size)\n",
    "→ Conv (find shapes)\n",
    "→ Pool (reduce size)\n",
    "→ Flatten → Dense layers → Prediction\n",
    "```\n",
    "\n",
    "## Challenge for Our Dataset\n",
    "\n",
    "**Problem**: CNNs typically need 50K+ samples. We only have 9,900.\n",
    "- Risk: Overfitting (memorizing training data instead of learning patterns)\n",
    "- Solution: Heavy regularization (dropout, weight decay)\n",
    "\n",
    "**Baseline to beat**: Random Forest = 93.0% accuracy, 0.910 F1-Macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import time\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Settings\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "label_names = {-1: 'Left', 0: 'Forward', 1: 'Right'}\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load temporal splits\n",
    "data_temporal = np.load('../data/splits_temporal.npz')\n",
    "data_random = np.load('../data/splits_random.npz')\n",
    "data_tfi = np.load('../data/splits_temporal_tfi.npz')\n",
    "\n",
    "# Temporal splits\n",
    "X_train_temp = data_temporal['X_train']\n",
    "y_train_temp = data_temporal['y_train']\n",
    "X_val = data_temporal['X_val']\n",
    "y_val = data_temporal['y_val']\n",
    "X_test = data_temporal['X_test']\n",
    "y_test = data_temporal['y_test']\n",
    "\n",
    "# Random splits (for comparison)\n",
    "X_train_rand = data_random['X_train']\n",
    "y_train_rand = data_random['y_train']\n",
    "X_val_rand = data_random['X_val']\n",
    "y_val_rand = data_random['y_val']\n",
    "X_test_rand = data_random['X_test']\n",
    "y_test_rand = data_random['y_test']\n",
    "\n",
    "# TFI-balanced\n",
    "X_train_tfi = data_tfi['X_train']\n",
    "y_train_tfi = data_tfi['y_train']\n",
    "\n",
    "# Load class weights\n",
    "class_weights = np.load('../data/class_weights.npy', allow_pickle=True).item()\n",
    "\n",
    "print(\"Temporal splits:\")\n",
    "print(f\"  Train: {X_train_temp.shape}, labels: {Counter(y_train_temp)}\")\n",
    "print(f\"  Val:   {X_val.shape}\")\n",
    "print(f\"  Test:  {X_test.shape}\")\n",
    "print(f\"\\nRandom splits:\")\n",
    "print(f\"  Train: {X_train_rand.shape}\")\n",
    "print(f\"\\nTFI-balanced:\")\n",
    "print(f\"  Train: {X_train_tfi.shape}, labels: {Counter(y_train_tfi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for CNN: (samples, height, width) → (samples, height, width, channels)\n",
    "# Grayscale has 1 channel\n",
    "X_train_temp_cnn = X_train_temp.reshape(-1, 64, 64, 1)\n",
    "X_val_cnn = X_val.reshape(-1, 64, 64, 1)\n",
    "X_test_cnn = X_test.reshape(-1, 64, 64, 1)\n",
    "\n",
    "X_train_rand_cnn = X_train_rand.reshape(-1, 64, 64, 1)\n",
    "X_val_rand_cnn = X_val_rand.reshape(-1, 64, 64, 1)\n",
    "X_test_rand_cnn = X_test_rand.reshape(-1, 64, 64, 1)\n",
    "\n",
    "X_train_tfi_cnn = X_train_tfi.reshape(-1, 64, 64, 1)\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "X_train_temp_cnn = X_train_temp_cnn / 255.0\n",
    "X_val_cnn = X_val_cnn / 255.0\n",
    "X_test_cnn = X_test_cnn / 255.0\n",
    "\n",
    "X_train_rand_cnn = X_train_rand_cnn / 255.0\n",
    "X_val_rand_cnn = X_val_rand_cnn / 255.0\n",
    "X_test_rand_cnn = X_test_rand_cnn / 255.0\n",
    "\n",
    "X_train_tfi_cnn = X_train_tfi_cnn / 255.0\n",
    "\n",
    "# Map labels: -1→0, 0→1, 1→2 (Keras expects labels starting from 0)\n",
    "y_train_temp_mapped = y_train_temp + 1\n",
    "y_val_mapped = y_val + 1\n",
    "y_test_mapped = y_test + 1\n",
    "\n",
    "y_train_rand_mapped = y_train_rand + 1\n",
    "y_val_rand_mapped = y_val_rand + 1\n",
    "y_test_rand_mapped = y_test_rand + 1\n",
    "\n",
    "y_train_tfi_mapped = y_train_tfi + 1\n",
    "\n",
    "print(f\"Prepared for CNN:\")\n",
    "print(f\"  X_train shape: {X_train_temp_cnn.shape}\")\n",
    "print(f\"  y_train mapped labels: {np.unique(y_train_temp_mapped)}\")\n",
    "print(f\"  Value range: [{X_train_temp_cnn.min():.2f}, {X_train_temp_cnn.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cnn(model, X_test, y_test, model_name=\"CNN\"):\n",
    "    \"\"\"\n",
    "    Evaluate CNN model.\n",
    "    y_test should be mapped (0, 1, 2)\n",
    "    \"\"\"\n",
    "    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_per_class = f1_score(y_test, y_pred, average=None, labels=[0, 1, 2])\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy:  {acc:.3f} ({acc*100:.1f}%)\")\n",
    "    print(f\"F1-Macro:  {f1_macro:.3f}\")\n",
    "    print(f\"\\nPer-class F1:\")\n",
    "    for idx, f1 in enumerate(f1_per_class):\n",
    "        original_label = idx - 1\n",
    "        print(f\"  {label_names[original_label]:8s}: {f1:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_left': f1_per_class[0],\n",
    "        'f1_forward': f1_per_class[1],\n",
    "        'f1_right': f1_per_class[2],\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Left', 'Forward', 'Right'],\n",
    "                yticklabels=['Left', 'Forward', 'Right'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title('Accuracy over Epochs', fontsize=12, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    axes[1].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Loss over Epochs', fontsize=12, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple CNN Architecture\n",
    "\n",
    "**Design philosophy**: Start simple to avoid overfitting\n",
    "\n",
    "**Architecture**:\n",
    "- Conv2D(32 filters, 3×3) + ReLU\n",
    "- MaxPooling(2×2) → reduces 64×64 to 32×32\n",
    "- Conv2D(64 filters, 3×3) + ReLU\n",
    "- MaxPooling(2×2) → reduces 32×32 to 16×16\n",
    "- Flatten → 16×16×64 = 16,384 features\n",
    "- Dense(128) + Dropout(0.5) + ReLU\n",
    "- Dense(3) + Softmax\n",
    "\n",
    "**Total parameters**: ~50K (small for CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_cnn():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and show architecture\n",
    "model = build_simple_cnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment 1: Simple CNN on Random Split\n",
    "\n",
    "**Purpose**: Demonstrate data leakage from temporal correlation.\n",
    "\n",
    "From EDA: consecutive frames have 0.89 correlation, frames within 100 steps still >0.5 correlated.\n",
    "\n",
    "Random split mixes temporally-close frames between train/test → inflated performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Simple CNN on RANDOM split...\")\n",
    "\n",
    "model_rand = build_simple_cnn()\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "start_time = time.time()\n",
    "history_rand = model_rand.fit(\n",
    "    X_train_rand_cnn, y_train_rand_mapped,\n",
    "    validation_data=(X_val_rand_cnn, y_val_rand_mapped),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "train_time_rand = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {train_time_rand:.1f} seconds\")\n",
    "\n",
    "results_cnn_rand = evaluate_cnn(model_rand, X_test_rand_cnn, y_test_rand_mapped, \n",
    "                                \"Simple CNN (Random Split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history_rand, \"Simple CNN - Random Split\")\n",
    "plot_confusion_matrix(results_cnn_rand['confusion_matrix'], \n",
    "                      \"Simple CNN (Random Split) - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Random Split Results\n",
    "\n",
    "*Fill in observations after running:*\n",
    "- Accuracy: ____%\n",
    "- F1-Macro: ____\n",
    "- Gap from baseline (Random Forest 93%): ____\n",
    "- Training vs validation gap: ____ (check for overfitting)\n",
    "\n",
    "*Expected: High accuracy due to data leakage (temporally similar frames in train/test)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment 2: Simple CNN on Temporal Split (No Balancing)\n",
    "\n",
    "**Purpose**: Realistic evaluation without data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Simple CNN on TEMPORAL split (original data)...\")\n",
    "\n",
    "model_temp_orig = build_simple_cnn()\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "start_time = time.time()\n",
    "history_temp_orig = model_temp_orig.fit(\n",
    "    X_train_temp_cnn, y_train_temp_mapped,\n",
    "    validation_data=(X_val_cnn, y_val_mapped),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "train_time_temp_orig = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {train_time_temp_orig:.1f} seconds\")\n",
    "\n",
    "results_temp_orig = evaluate_cnn(model_temp_orig, X_test_cnn, y_test_mapped,\n",
    "                                 \"Simple CNN (Temporal, No Balancing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history_temp_orig, \"Simple CNN - Temporal Split (Original)\")\n",
    "plot_confusion_matrix(results_temp_orig['confusion_matrix'],\n",
    "                      \"Simple CNN (Temporal) - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Temporal Split vs Random Split Comparison\n",
    "\n",
    "*Fill in after running both experiments:*\n",
    "\n",
    "| Metric | Random Split | Temporal Split | Gap |\n",
    "|--------|--------------|----------------|-----|\n",
    "| Accuracy | ____% | ____% | ____% |\n",
    "| F1-Macro | ____ | ____ | ____ |\n",
    "| F1-Right | ____ | ____ | ____ |\n",
    "\n",
    "*Expected: Random split shows 5-10% higher accuracy due to data leakage*\n",
    "\n",
    "**Conclusion**: *(Fill in based on results)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment 3: Simple CNN with Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Simple CNN with class weights...\")\n",
    "\n",
    "model_temp_weighted = build_simple_cnn()\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "start_time = time.time()\n",
    "history_temp_weighted = model_temp_weighted.fit(\n",
    "    X_train_temp_cnn, y_train_temp_mapped,\n",
    "    validation_data=(X_val_cnn, y_val_mapped),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "train_time_temp_weighted = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {train_time_temp_weighted:.1f} seconds\")\n",
    "\n",
    "results_temp_weighted = evaluate_cnn(model_temp_weighted, X_test_cnn, y_test_mapped,\n",
    "                                     \"Simple CNN (Temporal + Class Weights)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history_temp_weighted, \"Simple CNN - Class Weights\")\n",
    "plot_confusion_matrix(results_temp_weighted['confusion_matrix'],\n",
    "                      \"Simple CNN (Weighted) - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Class Weights Impact\n",
    "\n",
    "*Fill in after running:*\n",
    "\n",
    "| Metric | No Weights | With Weights | Change |\n",
    "|--------|------------|--------------|--------|\n",
    "| Accuracy | ____% | ____% | ____% |\n",
    "| F1-Right | ____ | ____ | ____ |\n",
    "\n",
    "**Observations**: *(Fill in based on results)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment 4: Simple CNN with TFI Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Simple CNN on TFI-balanced data...\")\n",
    "\n",
    "model_tfi = build_simple_cnn()\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "start_time = time.time()\n",
    "history_tfi = model_tfi.fit(\n",
    "    X_train_tfi_cnn, y_train_tfi_mapped,\n",
    "    validation_data=(X_val_cnn, y_val_mapped),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "train_time_tfi = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {train_time_tfi:.1f} seconds\")\n",
    "\n",
    "results_tfi = evaluate_cnn(model_tfi, X_test_cnn, y_test_mapped,\n",
    "                           \"Simple CNN (TFI Balanced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history_tfi, \"Simple CNN - TFI Balanced\")\n",
    "plot_confusion_matrix(results_tfi['confusion_matrix'],\n",
    "                      \"Simple CNN (TFI) - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: TFI Impact\n",
    "\n",
    "*Fill in after running:*\n",
    "\n",
    "Training set size: ____ samples (vs ____ original)\n",
    "\n",
    "| Metric | Original | TFI | Change |\n",
    "|--------|----------|-----|--------|\n",
    "| Accuracy | ____% | ____% | ____% |\n",
    "| F1-Right | ____ | ____ | ____ |\n",
    "\n",
    "**Observations**: *(Fill in)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experiment 5: Medium CNN (AlexNet-Style)\n",
    "\n",
    "**Purpose**: Test if deeper architecture improves performance.\n",
    "\n",
    "**Architecture** (simplified from 2020 project):\n",
    "- Conv2D(96, 5×5, stride=2) + ReLU + MaxPool\n",
    "- Conv2D(128, 3×3) + ReLU + MaxPool\n",
    "- Conv2D(256, 3×3) + ReLU\n",
    "- Flatten → Dense(512) + Dropout(0.5) + ReLU\n",
    "- Dense(3) + Softmax\n",
    "\n",
    "**Total parameters**: ~200K (4× more than simple CNN)\n",
    "\n",
    "**Risk**: May overfit on 9,900 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_medium_cnn():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(96, (5, 5), strides=2, activation='relu', input_shape=(64, 64, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_medium = build_medium_cnn()\n",
    "model_medium.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Medium CNN on temporal split + class weights...\")\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "start_time = time.time()\n",
    "history_medium = model_medium.fit(\n",
    "    X_train_temp_cnn, y_train_temp_mapped,\n",
    "    validation_data=(X_val_cnn, y_val_mapped),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "train_time_medium = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {train_time_medium:.1f} seconds\")\n",
    "\n",
    "results_medium = evaluate_cnn(model_medium, X_test_cnn, y_test_mapped,\n",
    "                              \"Medium CNN (Temporal + Weights)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history_medium, \"Medium CNN\")\n",
    "plot_confusion_matrix(results_medium['confusion_matrix'],\n",
    "                      \"Medium CNN - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Simple vs Medium Architecture\n",
    "\n",
    "*Fill in after running:*\n",
    "\n",
    "| Model | Params | Accuracy | F1-Macro | Train Time |\n",
    "|-------|--------|----------|----------|------------|\n",
    "| Simple CNN | ~50K | ____% | ____ | ____s |\n",
    "| Medium CNN | ~200K | ____% | ____ | ____s |\n",
    "\n",
    "**Check training curves**: Does medium CNN show more overfitting (train-val gap)?\n",
    "\n",
    "**Observations**: *(Fill in)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Experiment 6: Regularization Tuning\n",
    "\n",
    "**Purpose**: Find best regularization for simple CNN.\n",
    "\n",
    "**Test**: Heavier dropout (0.7 instead of 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_cnn_heavy_dropout():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.7),  # Heavy dropout\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Training Simple CNN with heavy dropout (0.7)...\")\n",
    "\n",
    "model_heavy = build_simple_cnn_heavy_dropout()\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "start_time = time.time()\n",
    "history_heavy = model_heavy.fit(\n",
    "    X_train_temp_cnn, y_train_temp_mapped,\n",
    "    validation_data=(X_val_cnn, y_val_mapped),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "train_time_heavy = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {train_time_heavy:.1f} seconds\")\n",
    "\n",
    "results_heavy = evaluate_cnn(model_heavy, X_test_cnn, y_test_mapped,\n",
    "                             \"Simple CNN (Heavy Dropout 0.7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history_heavy, \"Simple CNN - Heavy Dropout (0.7)\")\n",
    "plot_confusion_matrix(results_heavy['confusion_matrix'],\n",
    "                      \"Simple CNN (Heavy Dropout) - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Regularization Impact\n",
    "\n",
    "*Fill in:*\n",
    "\n",
    "| Dropout | Accuracy | F1-Macro | Overfitting (train-val gap) |\n",
    "|---------|----------|----------|----------------------------|\n",
    "| 0.5 | ____% | ____ | ____ |\n",
    "| 0.7 | ____% | ____ | ____ |\n",
    "\n",
    "**Observations**: *(Does heavier dropout reduce overfitting? Improve generalization?)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Overall Comparison: CNNs vs Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load baseline results\n",
    "with open('../results/baseline_results.json', 'r') as f:\n",
    "    baseline_results = json.load(f)\n",
    "\n",
    "# Create comprehensive comparison\n",
    "all_results = pd.DataFrame([\n",
    "    {'Model': 'Random Forest', 'Split': 'Temporal', 'Balance': 'Weights',\n",
    "     'Params': 'N/A', \n",
    "     'Acc': f\"{baseline_results['rf_weighted']['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{baseline_results['rf_weighted']['f1_macro']:.3f}\",\n",
    "     'F1-Right': f\"{baseline_results['rf_weighted']['f1_right']:.3f}\",\n",
    "     'Time': '~45s'},\n",
    "    \n",
    "    {'Model': 'Simple CNN', 'Split': 'Random', 'Balance': 'None',\n",
    "     'Params': '~50K',\n",
    "     'Acc': f\"{results_cnn_rand['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_cnn_rand['f1_macro']:.3f}\",\n",
    "     'F1-Right': f\"{results_cnn_rand['f1_right']:.3f}\",\n",
    "     'Time': f'{train_time_rand:.0f}s'},\n",
    "    \n",
    "    {'Model': 'Simple CNN', 'Split': 'Temporal', 'Balance': 'None',\n",
    "     'Params': '~50K',\n",
    "     'Acc': f\"{results_temp_orig['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_temp_orig['f1_macro']:.3f}\",\n",
    "     'F1-Right': f\"{results_temp_orig['f1_right']:.3f}\",\n",
    "     'Time': f'{train_time_temp_orig:.0f}s'},\n",
    "    \n",
    "    {'Model': 'Simple CNN', 'Split': 'Temporal', 'Balance': 'Weights',\n",
    "     'Params': '~50K',\n",
    "     'Acc': f\"{results_temp_weighted['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_temp_weighted['f1_macro']:.3f}\",\n",
    "     'F1-Right': f\"{results_temp_weighted['f1_right']:.3f}\",\n",
    "     'Time': f'{train_time_temp_weighted:.0f}s'},\n",
    "    \n",
    "    {'Model': 'Simple CNN', 'Split': 'Temporal', 'Balance': 'TFI',\n",
    "     'Params': '~50K',\n",
    "     'Acc': f\"{results_tfi['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_tfi['f1_macro']:.3f}\",\n",
    "     'F1-Right': f\"{results_tfi['f1_right']:.3f}\",\n",
    "     'Time': f'{train_time_tfi:.0f}s'},\n",
    "    \n",
    "    {'Model': 'Medium CNN', 'Split': 'Temporal', 'Balance': 'Weights',\n",
    "     'Params': '~200K',\n",
    "     'Acc': f\"{results_medium['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_medium['f1_macro']:.3f}\",\n",
    "     'F1-Right': f\"{results_medium['f1_right']:.3f}\",\n",
    "     'Time': f'{train_time_medium:.0f}s'},\n",
    "    \n",
    "    {'Model': 'Simple CNN Heavy', 'Split': 'Temporal', 'Balance': 'Weights',\n",
    "     'Params': '~50K',\n",
    "     'Acc': f\"{results_heavy['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_heavy['f1_macro']:.3f}\",\n",
    "     'F1-Right': f\"{results_heavy['f1_right']:.3f}\",\n",
    "     'Time': f'{train_time_heavy:.0f}s'}\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPLETE RESULTS: CNNs vs Baselines\")\n",
    "print(\"=\"*100)\n",
    "print(all_results.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# F1-Macro comparison\n",
    "models_list = ['RF-Weight', 'CNN-Random', 'CNN-Temp', 'CNN-Weight', 'CNN-TFI', 'Medium', 'Heavy']\n",
    "f1_macros = [\n",
    "    baseline_results['rf_weighted']['f1_macro'],\n",
    "    results_cnn_rand['f1_macro'],\n",
    "    results_temp_orig['f1_macro'],\n",
    "    results_temp_weighted['f1_macro'],\n",
    "    results_tfi['f1_macro'],\n",
    "    results_medium['f1_macro'],\n",
    "    results_heavy['f1_macro']\n",
    "]\n",
    "\n",
    "colors = ['green' if 'RF' in m else 'red' if 'Random' in m else 'steelblue' for m in models_list]\n",
    "axes[0].bar(range(len(models_list)), f1_macros, color=colors, alpha=0.7)\n",
    "axes[0].axhline(y=0.910, color='green', linestyle='--', linewidth=2, label='RF Baseline (0.910)')\n",
    "axes[0].set_xticks(range(len(models_list)))\n",
    "axes[0].set_xticklabels(models_list, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('F1-Macro', fontsize=12)\n",
    "axes[0].set_title('F1-Macro: CNNs vs Random Forest', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Training time comparison\n",
    "times = [45, train_time_rand, train_time_temp_orig, train_time_temp_weighted, \n",
    "         train_time_tfi, train_time_medium, train_time_heavy]\n",
    "axes[1].bar(range(len(models_list)), times, color=colors, alpha=0.7)\n",
    "axes[1].set_xticks(range(len(models_list)))\n",
    "axes[1].set_xticklabels(models_list, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "axes[1].set_title('Computational Cost', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Analysis: Do CNNs Beat Random Forest?\n",
    "\n",
    "*Fill in after seeing all results:*\n",
    "\n",
    "**Best CNN performance**: ____ accuracy, ____ F1-Macro  \n",
    "**Random Forest baseline**: 93.0% accuracy, 0.910 F1-Macro\n",
    "\n",
    "**Gap**: ____ (CNN - RF)\n",
    "\n",
    "**Interpretation**:\n",
    "- If gap > +2%: CNNs successfully learned spatial features worth the complexity\n",
    "- If gap ≈ 0±1%: Comparable performance, but CNNs have higher computational cost\n",
    "- If gap < -2%: CNNs overfit on small dataset, simpler models are better\n",
    "\n",
    "**Observations**: *(Fill in)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Error Analysis\n",
    "\n",
    "**Purpose**: Understand where models fail and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best CNN model for error analysis\n",
    "# Identify incorrectly classified samples\n",
    "y_pred_best = results_temp_weighted['predictions']  # Change based on best model\n",
    "y_test_original = y_test  # Original labels (-1, 0, 1)\n",
    "y_pred_original = y_pred_best - 1  # Map back from (0,1,2) to (-1,0,1)\n",
    "\n",
    "# Find misclassified samples\n",
    "errors = y_test_original != y_pred_original\n",
    "error_indices = np.where(errors)[0]\n",
    "\n",
    "print(f\"Total errors: {len(error_indices)} / {len(y_test)} ({len(error_indices)/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Categorize errors\n",
    "error_types = {}\n",
    "for idx in error_indices:\n",
    "    true_label = y_test_original[idx]\n",
    "    pred_label = y_pred_original[idx]\n",
    "    key = f\"{label_names[true_label]}→{label_names[pred_label]}\"\n",
    "    error_types[key] = error_types.get(key, 0) + 1\n",
    "\n",
    "print(\"\\nError breakdown:\")\n",
    "for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {error_type:20s}: {count:3d} errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize worst errors (most confident wrong predictions)\n",
    "# Get prediction probabilities\n",
    "probs = model_temp_weighted.predict(X_test_cnn, verbose=0)\n",
    "pred_confidence = np.max(probs, axis=1)\n",
    "\n",
    "# Find confident errors\n",
    "confident_errors = []\n",
    "for idx in error_indices:\n",
    "    confident_errors.append((idx, pred_confidence[idx]))\n",
    "\n",
    "# Sort by confidence (most confident errors first)\n",
    "confident_errors.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Show top 10 confident errors\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(10, len(confident_errors))):\n",
    "    idx, conf = confident_errors[i]\n",
    "    \n",
    "    axes[i].imshow(X_test[idx], cmap='gray')\n",
    "    true_label = y_test_original[idx]\n",
    "    pred_label = y_pred_original[idx]\n",
    "    \n",
    "    axes[i].set_title(\n",
    "        f\"True: {label_names[true_label]}\\nPred: {label_names[pred_label]} ({conf:.2f})\",\n",
    "        fontsize=10, color='red'\n",
    "    )\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Most Confident Errors (Wrong Predictions with High Confidence)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis Observations\n",
    "\n",
    "*After visualizing errors, categorize them:*\n",
    "\n",
    "**Common error patterns**:\n",
    "1. Temporal lag cases: *(Do errors show label mismatch issue we discussed?)*\n",
    "2. Ambiguous images: *(Images that genuinely look like they could go either way)*\n",
    "3. Systematic biases: *(Does model always confuse specific pairs?)*\n",
    "\n",
    "**Comparison to Random Forest**: *(Do they fail on same samples? Different samples?)*\n",
    "\n",
    "**Insights**: *(Fill in)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Data Leakage Analysis: Random vs Temporal Split\n",
    "\n",
    "**Purpose**: Quantify the impact of temporal correlation on evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare same model on random vs temporal splits\n",
    "leakage_analysis = pd.DataFrame([\n",
    "    {'Split Type': 'Random', \n",
    "     'Accuracy': f\"{results_cnn_rand['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_cnn_rand['f1_macro']:.3f}\"},\n",
    "    {'Split Type': 'Temporal',\n",
    "     'Accuracy': f\"{results_temp_orig['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{results_temp_orig['f1_macro']:.3f}\"}\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA LEAKAGE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(leakage_analysis.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate gap\n",
    "acc_gap = results_cnn_rand['accuracy'] - results_temp_orig['accuracy']\n",
    "f1_gap = results_cnn_rand['f1_macro'] - results_temp_orig['f1_macro']\n",
    "\n",
    "print(f\"\\nPerformance inflation from random split:\")\n",
    "print(f\"  Accuracy gap:  {acc_gap:.3f} ({acc_gap*100:.1f} percentage points)\")\n",
    "print(f\"  F1-Macro gap:  {f1_gap:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leakage Conclusions\n",
    "\n",
    "*Fill in after seeing gap:*\n",
    "\n",
    "**Observed gap**: ____% accuracy inflation from random split\n",
    "\n",
    "**Interpretation**:\n",
    "- If gap > 5%: Severe leakage - random split completely invalid\n",
    "- If gap 2-5%: Moderate leakage - temporal split essential\n",
    "- If gap < 2%: Minimal leakage (surprising given 0.89 frame correlation!)\n",
    "\n",
    "**Conclusion**: *(Fill in)*\n",
    "\n",
    "**This validates**: Temporal split methodology from EDA was correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Learning Curves (Dataset Size Analysis)\n",
    "\n",
    "**Purpose**: Understand if more data would help.\n",
    "\n",
    "Train on increasing fractions of training data: 20%, 40%, 60%, 80%, 100%\n",
    "\n",
    "**What to look for**:\n",
    "- If curves still rising at 100%: More data would help\n",
    "- If curves plateau: Model has saturated, more data won't help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on different data sizes\n",
    "fractions = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "learning_curve_results = []\n",
    "\n",
    "for frac in fractions:\n",
    "    n_samples = int(len(X_train_temp_cnn) * frac)\n",
    "    \n",
    "    X_subset = X_train_temp_cnn[:n_samples]\n",
    "    y_subset = y_train_temp_mapped[:n_samples]\n",
    "    \n",
    "    print(f\"\\nTraining on {frac*100:.0f}% data ({n_samples} samples)...\")\n",
    "    \n",
    "    model_lc = build_simple_cnn()\n",
    "    early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=0)\n",
    "    \n",
    "    history = model_lc.fit(\n",
    "        X_subset, y_subset,\n",
    "        validation_data=(X_val_cnn, y_val_mapped),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = np.argmax(model_lc.predict(X_test_cnn, verbose=0), axis=1)\n",
    "    acc = accuracy_score(y_test_mapped, y_pred)\n",
    "    f1 = f1_score(y_test_mapped, y_pred, average='macro')\n",
    "    \n",
    "    learning_curve_results.append({\n",
    "        'fraction': frac,\n",
    "        'n_samples': n_samples,\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1,\n",
    "        'train_acc': history.history['accuracy'][-1],\n",
    "        'val_acc': history.history['val_accuracy'][-1]\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test accuracy: {acc:.3f}, F1-Macro: {f1:.3f}\")\n",
    "\n",
    "print(\"\\nLearning curve analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "n_samples_list = [r['n_samples'] for r in learning_curve_results]\n",
    "test_accs = [r['accuracy'] for r in learning_curve_results]\n",
    "train_accs = [r['train_acc'] for r in learning_curve_results]\n",
    "val_accs = [r['val_acc'] for r in learning_curve_results]\n",
    "\n",
    "# Test accuracy vs dataset size\n",
    "axes[0].plot(n_samples_list, test_accs, marker='o', linewidth=2, markersize=8, label='Test')\n",
    "axes[0].axhline(y=0.930, color='green', linestyle='--', label='RF Baseline (93%)')\n",
    "axes[0].set_xlabel('Training Samples', fontsize=12)\n",
    "axes[0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[0].set_title('Learning Curve: Accuracy vs Dataset Size', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Train vs val accuracy (overfitting check)\n",
    "axes[1].plot(n_samples_list, train_accs, marker='o', linewidth=2, markersize=8, label='Train')\n",
    "axes[1].plot(n_samples_list, val_accs, marker='s', linewidth=2, markersize=8, label='Validation')\n",
    "axes[1].set_xlabel('Training Samples', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Overfitting Check: Train vs Val', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve Analysis\n",
    "\n",
    "*Fill in observations:*\n",
    "\n",
    "**Curve shape**: *(Flat? Still rising? Plateau?)*\n",
    "\n",
    "**Would more data help?**: *(Yes if curve rising, No if plateau)*\n",
    "\n",
    "**Overfitting severity**: *(Large train-val gap? Getting worse with more data?)*\n",
    "\n",
    "**Insights**: *(Fill in)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Results and Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model (change based on your results)\n",
    "best_model = model_temp_weighted  # Update this\n",
    "best_results = results_temp_weighted  # Update this\n",
    "best_name = \"Simple CNN (Temporal + Class Weights)\"  # Update this\n",
    "\n",
    "# Save model\n",
    "best_model.save('../models/best_cnn.keras')\n",
    "print(f\"Best model saved: {best_name}\")\n",
    "print(f\"  Accuracy: {best_results['accuracy']:.3f}\")\n",
    "print(f\"  F1-Macro: {best_results['f1_macro']:.3f}\")\n",
    "\n",
    "# Save all CNN results\n",
    "cnn_results = {\n",
    "    'cnn_random': {k: v.tolist() if isinstance(v, np.ndarray) else v \n",
    "                   for k, v in results_cnn_rand.items()},\n",
    "    'cnn_temporal_orig': {k: v.tolist() if isinstance(v, np.ndarray) else v \n",
    "                          for k, v in results_temp_orig.items()},\n",
    "    'cnn_temporal_weighted': {k: v.tolist() if isinstance(v, np.ndarray) else v \n",
    "                              for k, v in results_temp_weighted.items()},\n",
    "    'cnn_tfi': {k: v.tolist() if isinstance(v, np.ndarray) else v \n",
    "                for k, v in results_tfi.items()},\n",
    "    'cnn_medium': {k: v.tolist() if isinstance(v, np.ndarray) else v \n",
    "                   for k, v in results_medium.items()},\n",
    "    'cnn_heavy': {k: v.tolist() if isinstance(v, np.ndarray) else v \n",
    "                  for k, v in results_heavy.items()},\n",
    "    'learning_curve': learning_curve_results,\n",
    "    'best_model': best_name\n",
    "}\n",
    "\n",
    "with open('../results/cnn_results.json', 'w') as f:\n",
    "    json.dump(cnn_results, f, indent=2)\n",
    "\n",
    "print(\"\\nResults saved to: results/cnn_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Final Summary and Conclusions\n",
    "\n",
    "*Fill in after all experiments complete:*\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**1. CNN vs Random Forest**:\n",
    "- Best CNN: ____% accuracy, ____ F1-Macro\n",
    "- Random Forest: 93.0% accuracy, 0.910 F1-Macro\n",
    "- Gap: ____ \n",
    "- **Conclusion**: *(CNNs beat/match/lose to RF)*\n",
    "\n",
    "**2. Data Leakage from Random Split**:\n",
    "- Random split: ____% accuracy (inflated)\n",
    "- Temporal split: ____% accuracy (realistic)\n",
    "- Inflation: ____%\n",
    "- **Validates**: Temporal split methodology was essential\n",
    "\n",
    "**3. Architecture Complexity**:\n",
    "- Simple CNN (50K params): ____% \n",
    "- Medium CNN (200K params): ____%\n",
    "- **Observation**: *(Does deeper help or hurt?)*\n",
    "\n",
    "**4. Class Balancing**:\n",
    "- Original: ____ F1-Right\n",
    "- Class weights: ____ F1-Right\n",
    "- TFI: ____ F1-Right\n",
    "- **Impact**: *(Minimal/Moderate/Significant)*\n",
    "\n",
    "**5. Dataset Size**:\n",
    "- Learning curve: *(Still rising / Plateaued)*\n",
    "- More data would: *(Help / Not help)*\n",
    "\n",
    "### Honest Assessment\n",
    "\n",
    "*Choose the appropriate conclusion based on results:*\n",
    "\n",
    "**If CNN > 94%**:\n",
    "> \"CNNs successfully leveraged spatial hierarchies to beat traditional ML by X%, demonstrating that convolutional architectures can extract geometric features unavailable to pixel-based methods. The performance gain justifies the added complexity.\"\n",
    "\n",
    "**If CNN ≈ 93%**:\n",
    "> \"CNNs achieved comparable performance to Random Forest (93% vs 93%), but with 4× longer training time and less interpretability. For this specific problem with clean edge features, simpler tree-based methods are preferable.\"\n",
    "\n",
    "**If CNN < 92%**:\n",
    "> \"CNNs underperformed Random Forest (X% vs 93%) due to overfitting on the small 9,900-sample dataset. Despite heavy regularization, the parameter-to-sample ratio was too high. This demonstrates that deep learning is not always superior - dataset size matters more than model sophistication.\"\n",
    "\n",
    "### Paper Implications\n",
    "\n",
    "*(Fill in your honest take)*\n",
    "\n",
    "**Key message for paper**: *(What did we learn about CNNs vs traditional ML for this task?)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
