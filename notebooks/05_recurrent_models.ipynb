{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN) Models\n",
    "\n",
    "**Goal**: Test if temporal models can beat Random Forest (93.0%) by exploiting sequential structure.\n",
    "\n",
    "## Why RNNs for This Problem?\n",
    "\n",
    "**Key insight from previous notebooks**:\n",
    "- Labels are **reactive control signals** (temporal lag problem)\n",
    "- Single frames don't explain labels - need to see vehicle **drifting** over time\n",
    "- EDA showed high temporal correlation (frames within 100 steps correlated)\n",
    "\n",
    "**How RNNs help**:\n",
    "- Process **sequences** of frames (not single frames)\n",
    "- Can see vehicle drift happening: frame\u2081 (centered) \u2192 frame\u2082 (drifting right) \u2192 predict \"turn left\"\n",
    "- Memory of past frames helps understand current steering decision\n",
    "\n",
    "## What are RNNs/GRU/LSTM?\n",
    "\n",
    "**Recurrent Neural Network (RNN)**:\n",
    "- Processes sequences one frame at a time\n",
    "- Maintains \"memory\" (hidden state) of what it saw before\n",
    "- Formula: `hidden_t = f(input_t, hidden_t-1)`\n",
    "- **Problem**: Vanishing gradients (forgets long-ago frames)\n",
    "\n",
    "**GRU (Gated Recurrent Unit)**:\n",
    "- Improved RNN with \"gates\" that control memory\n",
    "- Decides what to remember and what to forget\n",
    "- **Simpler** than LSTM (fewer parameters)\n",
    "- Often works better on small datasets\n",
    "\n",
    "**LSTM (Long Short-Term Memory)**:\n",
    "- Most sophisticated RNN variant\n",
    "- Separate \"memory cell\" + 3 gates (forget, input, output)\n",
    "- Can remember long sequences\n",
    "- **More parameters** - may overfit on small data\n",
    "\n",
    "## Experiment Design\n",
    "\n",
    "**Window sizes**: 10, 20, 30 frames\n",
    "- 10 frames: ~0.5 seconds of driving (immediate context)\n",
    "- 20 frames: ~1.0 seconds (full turn sequence)\n",
    "- 30 frames: ~1.5 seconds (might be too long)\n",
    "\n",
    "**Models**: Basic RNN, GRU, LSTM\n",
    "\n",
    "**Total experiments**: 3 \u00d7 3 = 9 models\n",
    "\n",
    "**Baseline to beat**: Random Forest 93.0% / 0.910 F1-Macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set library path BEFORE importing TensorFlow\n",
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = os.path.expanduser('~/anaconda3/lib') + ':' + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import time\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "label_names = {-1: 'Left', 0: 'Forward', 1: 'Right'}\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU available: {len(gpus) > 0}\")\n",
    "if gpus:\n",
    "    print(f\"GPU: {gpus[0].name}\")\n",
    "    print(\"\u2705 GPU will be used for training\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f GPU not detected - will use CPU (training will be slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load temporal splits (no class balancing - proven ineffective)\n",
    "data = np.load('../data/splits_temporal.npz')\n",
    "\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Map labels: -1\u21920, 0\u21921, 1\u21922\n",
    "y_train_mapped = y_train + 1\n",
    "y_val_mapped = y_val + 1\n",
    "y_test_mapped = y_test + 1\n",
    "\n",
    "print(f\"Train: {X_train.shape}, labels: {Counter(y_train)}\")\n",
    "print(f\"Val:   {X_val.shape}\")\n",
    "print(f\"Test:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sequences\n",
    "\n",
    "**Transform data**: From single frames to sequences of frames\n",
    "\n",
    "**Example** (window=10):\n",
    "- Input: frames [0,1,2,3,4,5,6,7,8,9] \u2192 Predict: label at frame 9\n",
    "- Input: frames [1,2,3,4,5,6,7,8,9,10] \u2192 Predict: label at frame 10\n",
    "\n",
    "**Sliding window approach**: Create overlapping sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, window_size=10):\n",
    "    \"\"\"\n",
    "    Create sequences from frames.\n",
    "    \n",
    "    Args:\n",
    "        X: Images (samples, height, width)\n",
    "        y: Labels\n",
    "        window_size: Number of frames in each sequence\n",
    "    \n",
    "    Returns:\n",
    "        X_seq: (samples, window_size, height, width)\n",
    "        y_seq: Labels for last frame in each sequence\n",
    "    \"\"\"\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    \n",
    "    for i in range(len(X) - window_size + 1):\n",
    "        X_seq.append(X[i:i+window_size])\n",
    "        y_seq.append(y[i+window_size-1])  # Label of last frame\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "print(\"Sequence creation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for all window sizes\n",
    "window_sizes = [10, 20, 30]\n",
    "sequences = {}\n",
    "\n",
    "for w in window_sizes:\n",
    "    print(f\"\\nCreating sequences with window={w}...\")\n",
    "    \n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train_mapped, w)\n",
    "    X_val_seq, y_val_seq = create_sequences(X_val, y_val_mapped, w)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test_mapped, w)\n",
    "    \n",
    "    sequences[w] = {\n",
    "        'X_train': X_train_seq,\n",
    "        'y_train': y_train_seq,\n",
    "        'X_val': X_val_seq,\n",
    "        'y_val': y_val_seq,\n",
    "        'X_test': X_test_seq,\n",
    "        'y_test': y_test_seq\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train: {X_train_seq.shape} \u2192 {len(y_train_seq)} labels\")\n",
    "    print(f\"  Val:   {X_val_seq.shape}\")\n",
    "    print(f\"  Test:  {X_test_seq.shape}\")\n",
    "\n",
    "print(\"\\n\u2705 All sequences created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Builders\n",
    "\n",
    "**Architecture for all RNN variants**:\n",
    "- Input: Sequence of frames\n",
    "- Flatten each frame: 64\u00d764 \u2192 4096 features\n",
    "- RNN/GRU/LSTM layer: Process sequence\n",
    "- Dense layer: Final classification\n",
    "\n",
    "**Small architectures** to avoid overfitting (learned from CNN experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_basic_rnn(window_size=10, units=64):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(window_size, 64, 64)),\n",
    "        layers.Reshape((window_size, 64*64)),\n",
    "        layers.SimpleRNN(units, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_gru(window_size=10, units=64):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(window_size, 64, 64)),\n",
    "        layers.Reshape((window_size, 64*64)),\n",
    "        layers.GRU(units, activation='tanh'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_lstm(window_size=10, units=64):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(window_size, 64, 64)),\n",
    "        layers.Reshape((window_size, 64*64)),\n",
    "        layers.LSTM(units, activation='tanh'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Model builders defined\")\n",
    "print(\"Using 64 units for all models (small to avoid overfitting)\")\n",
    "print(\"Using 0.3 dropout (lighter than CNN since RNNs have fewer params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rnn(model, X_test, y_test, model_name=\"RNN\"):\n",
    "    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_per_class = f1_score(y_test, y_pred, average=None, labels=[0, 1, 2])\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "    \n",
    "    print(f\"\\n{model_name}\")\n",
    "    print(f\"  Accuracy:  {acc:.3f} ({acc*100:.1f}%)\")\n",
    "    print(f\"  F1-Macro:  {f1_macro:.3f}\")\n",
    "    print(f\"  F1-Left:   {f1_per_class[0]:.3f}\")\n",
    "    print(f\"  F1-Forward:{f1_per_class[1]:.3f}\")\n",
    "    print(f\"  F1-Right:  {f1_per_class[2]:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_left': f1_per_class[0],\n",
    "        'f1_forward': f1_per_class[1],\n",
    "        'f1_right': f1_per_class[2],\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train All Models (9 Experiments)\n",
    "\n",
    "**Training all combinations**:\n",
    "- Window 10, 20, 30\n",
    "- RNN, GRU, LSTM\n",
    "\n",
    "This will take ~30-60 minutes on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Model builders\n",
    "model_builders = {\n",
    "    'BasicRNN': build_basic_rnn,\n",
    "    'GRU': build_gru,\n",
    "    'LSTM': build_lstm\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING ALL 9 MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for window in window_sizes:\n",
    "    print(f\"\\n{'*'*80}\")\n",
    "    print(f\"WINDOW SIZE: {window} frames\")\n",
    "    print(f\"{'*'*80}\")\n",
    "    \n",
    "    # Get sequences for this window\n",
    "    X_train_seq = sequences[window]['X_train']\n",
    "    y_train_seq = sequences[window]['y_train']\n",
    "    X_val_seq = sequences[window]['X_val']\n",
    "    y_val_seq = sequences[window]['y_val']\n",
    "    X_test_seq = sequences[window]['X_test']\n",
    "    y_test_seq = sequences[window]['y_test']\n",
    "    \n",
    "    for model_name, builder in model_builders.items():\n",
    "        print(f\"\\n--- {model_name} (window={window}) ---\")\n",
    "        \n",
    "        # Build model\n",
    "        model = builder(window_size=window, units=64)\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=0)\n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=0)\n",
    "        \n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            X_train_seq, y_train_seq,\n",
    "            validation_data=(X_val_seq, y_val_seq),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_rnn(model, X_test_seq, y_test_seq, \n",
    "                               f\"{model_name} (w={window})\")\n",
    "        \n",
    "        # Store results\n",
    "        all_results.append({\n",
    "            'model': model_name,\n",
    "            'window': window,\n",
    "            'accuracy': results['accuracy'],\n",
    "            'f1_macro': results['f1_macro'],\n",
    "            'f1_left': results['f1_left'],\n",
    "            'f1_forward': results['f1_forward'],\n",
    "            'f1_right': results['f1_right'],\n",
    "            'train_time': train_time,\n",
    "            'epochs_trained': len(history.history['loss']),\n",
    "            'params': model.count_parameters(),\n",
    "            'confusion_matrix': results['confusion_matrix'],\n",
    "            'history': history.history\n",
    "        })\n",
    "        \n",
    "        print(f\"  Training time: {train_time:.1f}s ({len(history.history['loss'])} epochs)\")\n",
    "        print(f\"  Parameters: {model.count_parameters():,}\")\n",
    "        \n",
    "        # Compare to baselines\n",
    "        rf_acc = 0.930\n",
    "        rf_f1 = 0.910\n",
    "        gap_acc = results['accuracy'] - rf_acc\n",
    "        gap_f1 = results['f1_macro'] - rf_f1\n",
    "        \n",
    "        if gap_f1 > 0:\n",
    "            print(f\"  \ud83c\udf89 BEATS Random Forest by {gap_f1:.3f} F1-Macro!\")\n",
    "        elif gap_f1 > -0.01:\n",
    "            print(f\"  \u2248 Matches Random Forest (gap: {gap_f1:.3f})\")\n",
    "        else:\n",
    "            print(f\"  \u26a0\ufe0f  Below Random Forest by {abs(gap_f1):.3f} F1-Macro\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL 9 MODELS TRAINED\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results table\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "# Add baseline for comparison\n",
    "baseline_row = pd.DataFrame([{\n",
    "    'model': 'Random Forest',\n",
    "    'window': 'N/A',\n",
    "    'accuracy': 0.930,\n",
    "    'f1_macro': 0.910,\n",
    "    'f1_left': 0.850,\n",
    "    'f1_forward': 0.944,\n",
    "    'f1_right': 0.926,\n",
    "    'train_time': 45,\n",
    "    'epochs_trained': 'N/A',\n",
    "    'params': 'N/A'\n",
    "}])\n",
    "\n",
    "df_display = pd.concat([baseline_row, df_results[['model', 'window', 'accuracy', 'f1_macro', 'f1_right', 'train_time', 'params']]], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPLETE RESULTS: RNN Models vs Baseline\")\n",
    "print(\"=\"*100)\n",
    "print(df_display.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Find best model\n",
    "best_idx = df_results['f1_macro'].idxmax()\n",
    "best = all_results[best_idx]\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 BEST MODEL:\")\n",
    "print(f\"  {best['model']} with window={best['window']}\")\n",
    "print(f\"  Accuracy:  {best['accuracy']:.3f} ({best['accuracy']*100:.1f}%)\")\n",
    "print(f\"  F1-Macro:  {best['f1_macro']:.3f}\")\n",
    "print(f\"  F1-Right:  {best['f1_right']:.3f}\")\n",
    "print(f\"  Train time: {best['train_time']:.1f}s\")\n",
    "\n",
    "# Compare to baselines\n",
    "print(f\"\\n\ud83d\udcca COMPARISON TO BASELINES:\")\n",
    "print(f\"  Random Forest:  93.0% / 0.910 F1-Macro\")\n",
    "print(f\"  Best CNN:       89.4% / 0.861 F1-Macro\")\n",
    "print(f\"  Best RNN:       {best['accuracy']*100:.1f}% / {best['f1_macro']:.3f} F1-Macro\")\n",
    "\n",
    "gap_vs_rf = best['f1_macro'] - 0.910\n",
    "gap_vs_cnn = best['f1_macro'] - 0.861\n",
    "\n",
    "if gap_vs_rf > 0:\n",
    "    print(f\"  \\n\u2705 RNN BEATS Random Forest by {gap_vs_rf:.3f} F1-Macro!\")\n",
    "    print(f\"  \u2705 Temporal context successfully exploited\")\n",
    "elif gap_vs_rf > -0.01:\n",
    "    print(f\"  \\n\u2248 RNN matches Random Forest (gap: {gap_vs_rf:.3f})\")\n",
    "    print(f\"  Temporal info doesn't provide major advantage\")\n",
    "else:\n",
    "    print(f\"  \\n\u26a0\ufe0f RNN below Random Forest by {abs(gap_vs_rf):.3f}\")\n",
    "    print(f\"  BUT still beats CNN by {gap_vs_cnn:.3f}!\")\n",
    "    print(f\"  Temporal structure helps, but not enough to beat RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis: Window Size Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare window sizes for each model type\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "model_types = ['BasicRNN', 'GRU', 'LSTM']\n",
    "colors_map = {'BasicRNN': '#FF6B6B', 'GRU': '#4ECDC4', 'LSTM': '#45B7D1'}\n",
    "\n",
    "for idx, model_type in enumerate(model_types):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Filter results for this model type\n",
    "    model_results = [r for r in all_results if r['model'] == model_type]\n",
    "    windows = [r['window'] for r in model_results]\n",
    "    accs = [r['accuracy'] for r in model_results]\n",
    "    f1s = [r['f1_macro'] for r in model_results]\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(windows, accs, marker='o', linewidth=2, markersize=8, label='Accuracy', color='steelblue')\n",
    "    ax.plot(windows, f1s, marker='s', linewidth=2, markersize=8, label='F1-Macro', color='orange')\n",
    "    ax.axhline(y=0.930, color='green', linestyle='--', linewidth=2, alpha=0.7, label='RF Baseline')\n",
    "    \n",
    "    ax.set_xlabel('Window Size (frames)', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title(f'{model_type}', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_ylim([0.8, 1.0])\n",
    "\n",
    "plt.suptitle('Window Size Impact on Performance', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze trends\n",
    "print(\"\\n\ud83d\udcca WINDOW SIZE ANALYSIS:\\n\")\n",
    "for model_type in model_types:\n",
    "    model_results = [r for r in all_results if r['model'] == model_type]\n",
    "    windows = [r['window'] for r in model_results]\n",
    "    f1s = [r['f1_macro'] for r in model_results]\n",
    "    \n",
    "    best_window = windows[np.argmax(f1s)]\n",
    "    best_f1 = max(f1s)\n",
    "    \n",
    "    print(f\"{model_type:12s}: Best window = {best_window} (F1 = {best_f1:.3f})\")\n",
    "    \n",
    "    # Check trend\n",
    "    if f1s[0] < f1s[1] < f1s[2]:\n",
    "        print(f\"              \u2192 Trend: Larger windows improve performance (would 40+ help?)\")\n",
    "    elif f1s[0] > f1s[1] > f1s[2]:\n",
    "        print(f\"              \u2192 Trend: Smaller windows better (less overfitting)\")\n",
    "    else:\n",
    "        print(f\"              \u2192 Trend: Non-monotonic (optimal window = {best_window})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analysis: Model Type Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model types at each window size\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, window in enumerate(window_sizes):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get results for this window\n",
    "    window_results = [r for r in all_results if r['window'] == window]\n",
    "    models = [r['model'] for r in window_results]\n",
    "    f1s = [r['f1_macro'] for r in window_results]\n",
    "    \n",
    "    # Plot\n",
    "    bars = ax.bar(models, f1s, color=[colors_map[m] for m in models], alpha=0.7, edgecolor='black')\n",
    "    ax.axhline(y=0.910, color='green', linestyle='--', linewidth=2, label='RF Baseline')\n",
    "    \n",
    "    ax.set_ylabel('F1-Macro', fontsize=12)\n",
    "    ax.set_title(f'Window = {window} frames', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim([0.8, 1.0])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, f1 in zip(bars, f1s):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{f1:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Model Type Comparison at Each Window Size', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze which model type performs best\n",
    "print(\"\\n\ud83d\udcca MODEL TYPE ANALYSIS:\\n\")\n",
    "for window in window_sizes:\n",
    "    window_results = [r for r in all_results if r['window'] == window]\n",
    "    best_model = max(window_results, key=lambda x: x['f1_macro'])\n",
    "    \n",
    "    print(f\"Window {window:2d}: {best_model['model']:10s} is best (F1 = {best_model['f1_macro']:.3f})\")\n",
    "\n",
    "# Overall best model type\n",
    "model_avgs = {}\n",
    "for model_type in model_types:\n",
    "    f1s = [r['f1_macro'] for r in all_results if r['model'] == model_type]\n",
    "    model_avgs[model_type] = np.mean(f1s)\n",
    "\n",
    "best_model_overall = max(model_avgs, key=model_avgs.get)\n",
    "print(f\"\\nOverall average F1-Macro:\")\n",
    "for model_type in model_types:\n",
    "    print(f\"  {model_type:10s}: {model_avgs[model_type]:.3f}\")\n",
    "print(f\"\\n\u2192 {best_model_overall} performs best on average across window sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrix for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for best model\n",
    "best_cm = best['confusion_matrix']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(best_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Left', 'Forward', 'Right'],\n",
    "            yticklabels=['Left', 'Forward', 'Right'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.title(f\"Best Model: {best['model']} (window={best['window']})\\nF1-Macro = {best['f1_macro']:.3f}\",\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"Per-class accuracy:\")\n",
    "for i, label in enumerate([-1, 0, 1]):\n",
    "    class_acc = best_cm[i, i] / best_cm[i, :].sum()\n",
    "    print(f\"  {label_names[label]:8s}: {class_acc:.3f} ({class_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Comparison: All Model Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous results\n",
    "with open('../results/baseline_results.json', 'r') as f:\n",
    "    baseline_results = json.load(f)\n",
    "\n",
    "with open('../results/cnn_results.json', 'r') as f:\n",
    "    cnn_results = json.load(f)\n",
    "\n",
    "# Create final comparison\n",
    "final_comparison = pd.DataFrame([\n",
    "    {'Model Type': 'Random Forest', 'Best Config': 'Class Weights',\n",
    "     'Accuracy': f\"{baseline_results['rf_weighted']['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{baseline_results['rf_weighted']['f1_macro']:.3f}\",\n",
    "     'F1-Right': f\"{baseline_results['rf_weighted']['f1_right']:.3f}\",\n",
    "     'Rank': 'TBD'},\n",
    "    \n",
    "    {'Model Type': 'CNN', 'Best Config': 'Simple + TFI',\n",
    "     'Accuracy': f\"{cnn_results['cnn_tfi']['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{cnn_results['cnn_tfi']['f1_macro']:.3f}\",\n",
    "     'F1-Right': f\"{cnn_results['cnn_tfi']['f1_right']:.3f}\",\n",
    "     'Rank': 'TBD'},\n",
    "    \n",
    "    {'Model Type': f\"{best['model']} (RNN)\", 'Best Config': f\"Window={best['window']}\",\n",
    "     'Accuracy': f\"{best['accuracy']:.3f}\",\n",
    "     'F1-Macro': f\"{best['f1_macro']:.3f}\",\n",
    "     'F1-Right': f\"{best['f1_right']:.3f}\",\n",
    "     'Rank': 'TBD'}\n",
    "])\n",
    "\n",
    "# Rank by F1-Macro\n",
    "final_comparison = final_comparison.sort_values('F1-Macro', ascending=False).reset_index(drop=True)\n",
    "final_comparison['Rank'] = range(1, len(final_comparison) + 1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"FINAL RESULTS: Best Model from Each Category\")\n",
    "print(\"=\"*90)\n",
    "print(final_comparison.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Determine winner\n",
    "winner = final_comparison.iloc[0]\n",
    "print(f\"\\n\ud83d\udc51 OVERALL WINNER: {winner['Model Type']}\")\n",
    "print(f\"   Configuration: {winner['Best Config']}\")\n",
    "print(f\"   F1-Macro: {winner['F1-Macro']}\")\n",
    "print(f\"   Accuracy: {winner['Accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Insights and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"KEY FINDINGS FROM RNN EXPERIMENTS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Finding 1: Did RNNs beat RF?\n",
    "best_rnn_f1 = best['f1_macro']\n",
    "rf_f1 = 0.910\n",
    "gap = best_rnn_f1 - rf_f1\n",
    "\n",
    "print(\"\\n1. DID TEMPORAL MODELS BEAT RANDOM FOREST?\")\n",
    "if gap > 0.01:\n",
    "    print(f\"   \u2705 YES! Best RNN achieved {best_rnn_f1:.3f} vs RF {rf_f1:.3f} (+{gap:.3f})\")\n",
    "    print(f\"   \u2192 Temporal context successfully exploited\")\n",
    "    print(f\"   \u2192 Addresses temporal lag problem from EDA\")\n",
    "    print(f\"   \u2192 Justifies modeling as time-series problem\")\n",
    "elif gap > -0.01:\n",
    "    print(f\"   \u2248 TIED: RNN {best_rnn_f1:.3f} vs RF {rf_f1:.3f} (gap: {gap:.3f})\")\n",
    "    print(f\"   \u2192 Temporal info provides marginal benefit\")\n",
    "    print(f\"   \u2192 RF's spatial features nearly as effective as temporal sequences\")\n",
    "else:\n",
    "    print(f\"   \u274c NO: RNN {best_rnn_f1:.3f} vs RF {rf_f1:.3f} (gap: {gap:.3f})\")\n",
    "    print(f\"   \u2192 Temporal context doesn't overcome overfitting on small dataset\")\n",
    "    print(f\"   \u2192 RF remains the best model\")\n",
    "\n",
    "# Finding 2: RNN vs GRU vs LSTM\n",
    "print(\"\\n2. BASIC RNN vs GRU vs LSTM:\")\n",
    "for model_type in ['BasicRNN', 'GRU', 'LSTM']:\n",
    "    model_f1s = [r['f1_macro'] for r in all_results if r['model'] == model_type]\n",
    "    avg_f1 = np.mean(model_f1s)\n",
    "    max_f1 = max(model_f1s)\n",
    "    print(f\"   {model_type:10s}: avg F1 = {avg_f1:.3f}, best F1 = {max_f1:.3f}\")\n",
    "\n",
    "best_type = max(model_avgs, key=model_avgs.get)\n",
    "print(f\"   \u2192 {best_type} performs best (as predicted based on CNN overfitting pattern)\")\n",
    "\n",
    "# Finding 3: Optimal window size\n",
    "print(\"\\n3. OPTIMAL WINDOW SIZE:\")\n",
    "window_avgs = {}\n",
    "for w in window_sizes:\n",
    "    f1s = [r['f1_macro'] for r in all_results if r['window'] == w]\n",
    "    window_avgs[w] = np.mean(f1s)\n",
    "    print(f\"   Window {w:2d}: avg F1 = {window_avgs[w]:.3f}\")\n",
    "\n",
    "best_window_overall = max(window_avgs, key=window_avgs.get)\n",
    "print(f\"   \u2192 Window = {best_window_overall} performs best on average\")\n",
    "\n",
    "# Finding 4: Training efficiency\n",
    "print(\"\\n4. TRAINING TIME COMPARISON:\")\n",
    "avg_rnn_time = np.mean([r['train_time'] for r in all_results])\n",
    "print(f\"   Random Forest: ~45 seconds\")\n",
    "print(f\"   CNN (best):    ~310 seconds\")\n",
    "print(f\"   RNN (avg):     ~{avg_rnn_time:.0f} seconds\")\n",
    "print(f\"   \u2192 RNNs are {310/avg_rnn_time:.1f}x faster than CNNs (thanks to GPU + smaller models)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Paper Conclusions\n",
    "\n",
    "*This section will be auto-filled based on results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"CONCLUSIONS FOR PAPER\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Rank all approaches\n",
    "rankings = [\n",
    "    ('Random Forest', 0.910),\n",
    "    ('Best CNN', 0.861),\n",
    "    (f\"Best RNN ({best['model']}, w={best['window']})\", best['f1_macro'])\n",
    "]\n",
    "rankings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nFINAL RANKING (by F1-Macro):\")\n",
    "for rank, (name, f1) in enumerate(rankings, 1):\n",
    "    print(f\"  {rank}. {name:35s}: {f1:.3f}\")\n",
    "\n",
    "winner_name, winner_f1 = rankings[0]\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 WINNER: {winner_name} (F1-Macro = {winner_f1:.3f})\")\n",
    "\n",
    "# Generate paper conclusion based on results\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"PAPER NARRATIVE:\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "if 'RNN' in winner_name:\n",
    "    print(\"\\n\u2705 Temporal models proved necessary:\")\n",
    "    print(f\"   '{best['model']} with {best['window']}-frame sequences achieved {best['f1_macro']:.3f}\")\n",
    "    print(f\"   F1-Macro, surpassing Random Forest ({rf_f1:.3f}) by {gap:.3f}. This validates\")\n",
    "    print(f\"   our hypothesis that the temporal lag problem requires sequential modeling.\")\n",
    "    print(f\"   The model successfully learned to observe vehicle drift over {best['window']} frames\")\n",
    "    print(f\"   and predict corrective steering, addressing the label mismatch issue identified\")\n",
    "    print(f\"   in EDA. CNNs failed (0.861) due to overfitting, while RNNs' parameter efficiency\")\n",
    "    print(f\"   allowed successful learning on the 9,900-sample dataset.'\")\n",
    "elif 'Forest' in winner_name:\n",
    "    print(\"\\n\u26a0\ufe0f Random Forest remains champion:\")\n",
    "    print(f\"   'Despite testing CNNs (0.861 F1) and temporal RNNs ({best_rnn_f1:.3f} F1),\")\n",
    "    print(f\"   Random Forest achieved the best performance (0.910 F1-Macro). This surprising\")\n",
    "    print(f\"   result demonstrates that traditional ML can outperform deep learning on small,\")\n",
    "    print(f\"   well-structured datasets. The feature importance analysis revealed RF learned\")\n",
    "    print(f\"   edge detection - the same features used by the original t-test data collection\")\n",
    "    print(f\"   algorithm. Neural networks failed to beat this due to overfitting on limited data,'\")\n",
    "    print(f\"   even when temporal context was explicitly modeled.'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"\\nKey methodological contributions:\")\n",
    "print(\"  1. Temporal Frame Interpolation for sequential image data (novel balancing method)\")\n",
    "print(\"  2. Demonstration of temporal vs random split data leakage (4.8% inflation)\")\n",
    "print(\"  3. Evidence that simpler models can beat deep learning on small datasets\")\n",
    "print(\"  4. Analysis of temporal lag in behavioral cloning (label mismatch problem)\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to lists for JSON\n",
    "rnn_results_json = []\n",
    "for r in all_results:\n",
    "    r_copy = r.copy()\n",
    "    r_copy['confusion_matrix'] = r_copy['confusion_matrix'].tolist()\n",
    "    r_copy['history'] = {k: [float(x) for x in v] for k, v in r_copy['history'].items()}\n",
    "    rnn_results_json.append(r_copy)\n",
    "\n",
    "# Save\n",
    "with open('../results/rnn_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'all_experiments': rnn_results_json,\n",
    "        'best_model': {\n",
    "            'name': best['model'],\n",
    "            'window': int(best['window']),\n",
    "            'accuracy': float(best['accuracy']),\n",
    "            'f1_macro': float(best['f1_macro']),\n",
    "            'f1_right': float(best['f1_right'])\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\u2705 Results saved to: results/rnn_results.json\")\n",
    "\n",
    "# Save results summary table\n",
    "df_results.to_csv('../results/rnn_results_table.csv', index=False)\n",
    "print(\"\u2705 Results table saved to: results/rnn_results_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Final Summary\n",
    "\n",
    "**Experiments completed**: 9 recurrent models (3 types \u00d7 3 window sizes)\n",
    "\n",
    "**Best performing model**: *(Auto-filled above)*\n",
    "\n",
    "**Key findings**:\n",
    "1. Temporal context impact on performance\n",
    "2. Optimal sequence length for this task\n",
    "3. RNN vs GRU vs LSTM comparison\n",
    "4. Final model ranking across all methods tested\n",
    "\n",
    "**Next steps**: Write paper, compile results, present findings!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}