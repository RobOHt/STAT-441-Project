{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random vs Temporal Splits: Model Performance Comparison\n",
        "\n",
        "This notebook compares the performance of Logistic Regression and k-Nearest Neighbors (kNN) models when trained on:\n",
        "1. **Random Split**: Data randomly shuffled and split (70% train, 15% val, 15% test)\n",
        "2. **Temporal Split**: Data split sequentially by time (60% train, 20% val, 20% test)\n",
        "\n",
        "**Key Question**: How does data leakage from temporal correlation affect model performance?\n",
        "\n",
        "**Context**: This dataset consists of consecutive frames from a car driving on a track. Consecutive frames are highly similar, which means:\n",
        "- Random splits may leak information (test frames similar to training frames)\n",
        "- Temporal splits provide a more realistic evaluation (test on future frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    balanced_accuracy_score, f1_score, precision_score, recall_score\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import shared utilities\n",
        "from utils import (\n",
        "    CLASSES, RANDOM_STATE, CLASS_COLORS, evaluate_model\n",
        ")\n",
        "from pathlib import Path\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(\"Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Pre-split Data\n",
        "\n",
        "Load the random and temporal splits that were created in the EDA notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "DATA_DIR = Path('../../data')\n",
        "\n",
        "# Load random splits\n",
        "print(\"Loading random splits...\")\n",
        "random_data = np.load(DATA_DIR / 'splits_random.npz', allow_pickle=True)\n",
        "X_train_rand = random_data['X_train']\n",
        "y_train_rand = random_data['y_train']\n",
        "X_val_rand = random_data['X_val']\n",
        "y_val_rand = random_data['y_val']\n",
        "X_test_rand = random_data['X_test']\n",
        "y_test_rand = random_data['y_test']\n",
        "\n",
        "# Load temporal splits\n",
        "print(\"Loading temporal splits...\")\n",
        "temporal_data = np.load(DATA_DIR / 'splits_temporal.npz', allow_pickle=True)\n",
        "X_train_temp = temporal_data['X_train']\n",
        "y_train_temp = temporal_data['y_train']\n",
        "X_val_temp = temporal_data['X_val']\n",
        "y_val_temp = temporal_data['y_val']\n",
        "X_test_temp = temporal_data['X_test']\n",
        "y_test_temp = temporal_data['y_test']\n",
        "\n",
        "# Check and handle data shape - flatten if 3D (images)\n",
        "def ensure_2d(X):\n",
        "    \"\"\"Ensure X is 2D by flattening if needed\"\"\"\n",
        "    if len(X.shape) > 2:\n",
        "        print(f\"  Flattening from shape {X.shape} to ({X.shape[0]}, {np.prod(X.shape[1:])})\")\n",
        "        return X.reshape(X.shape[0], -1)\n",
        "    return X\n",
        "\n",
        "X_train_rand = ensure_2d(X_train_rand)\n",
        "X_val_rand = ensure_2d(X_val_rand)\n",
        "X_test_rand = ensure_2d(X_test_rand)\n",
        "X_train_temp = ensure_2d(X_train_temp)\n",
        "X_val_temp = ensure_2d(X_val_temp)\n",
        "X_test_temp = ensure_2d(X_test_temp)\n",
        "\n",
        "print(f\"\\nRandom Split:\")\n",
        "print(f\"  Train: {len(X_train_rand)} samples, shape: {X_train_rand.shape}\")\n",
        "print(f\"  Val:   {len(X_val_rand)} samples, shape: {X_val_rand.shape}\")\n",
        "print(f\"  Test:  {len(X_test_rand)} samples, shape: {X_test_rand.shape}\")\n",
        "\n",
        "print(f\"\\nTemporal Split:\")\n",
        "print(f\"  Train: {len(X_train_temp)} samples, shape: {X_train_temp.shape}\")\n",
        "print(f\"  Val:   {len(X_val_temp)} samples, shape: {X_val_temp.shape}\")\n",
        "print(f\"  Test:  {len(X_test_temp)} samples, shape: {X_test_temp.shape}\")\n",
        "\n",
        "print(f\"\\nFeature dimensions: {X_train_rand.shape[1]}\")\n",
        "\n",
        "# Check label format and convert if needed\n",
        "def convert_labels(y, label_mapping={-1: 'left', 0: 'forward', 1: 'right'}):\n",
        "    \"\"\"Convert numeric labels to class names if needed\"\"\"\n",
        "    # Convert to regular Python types for easier handling\n",
        "    y = np.asarray(y)\n",
        "    \n",
        "    # Check if labels are strings (including numpy string types)\n",
        "    if y.dtype.kind in ['U', 'S', 'O']:  # Unicode, byte string, or object\n",
        "        # Try to convert string labels to numeric first\n",
        "        try:\n",
        "            y_numeric = y.astype(str).astype(float).astype(int)\n",
        "            unique_vals = set(y_numeric)\n",
        "            if unique_vals.issubset(set(label_mapping.keys())):\n",
        "                print(f\"  Converting numeric string labels {unique_vals} to class names\")\n",
        "                return np.array([label_mapping[int(val)] for val in y_numeric])\n",
        "        except (ValueError, TypeError):\n",
        "            pass\n",
        "        \n",
        "        # Check if already class names\n",
        "        y_str = np.array([str(val) for val in y])\n",
        "        if set(y_str).issubset(set(CLASSES)):\n",
        "            return y_str\n",
        "    \n",
        "    # If numeric, convert to class names\n",
        "    elif y.dtype.kind in ['i', 'f']:  # Integer or float\n",
        "        unique_vals = set(y.astype(int))\n",
        "        if unique_vals.issubset(set(label_mapping.keys())):\n",
        "            print(f\"  Converting numeric labels {unique_vals} to class names\")\n",
        "            return np.array([label_mapping[int(val)] for val in y])\n",
        "    \n",
        "    # If already class names, return as is\n",
        "    y_str = np.array([str(val) for val in y])\n",
        "    if set(y_str).issubset(set(CLASSES)):\n",
        "        return y_str\n",
        "    \n",
        "    return y\n",
        "\n",
        "# Convert labels to class names\n",
        "print(\"\\nConverting labels to class names...\")\n",
        "y_train_rand = convert_labels(y_train_rand)\n",
        "y_val_rand = convert_labels(y_val_rand)\n",
        "y_test_rand = convert_labels(y_test_rand)\n",
        "y_train_temp = convert_labels(y_train_temp)\n",
        "y_val_temp = convert_labels(y_val_temp)\n",
        "y_test_temp = convert_labels(y_test_temp)\n",
        "\n",
        "print(f\"\\nLabel check:\")\n",
        "print(f\"  Random train labels: {np.unique(y_train_rand, return_counts=True)}\")\n",
        "print(f\"  Temporal train labels: {np.unique(y_train_temp, return_counts=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocess Data\n",
        "\n",
        "Standardize features and encode labels for both splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "le.fit(CLASSES)\n",
        "\n",
        "# Random split preprocessing\n",
        "scaler_rand = StandardScaler()\n",
        "X_train_rand_scaled = scaler_rand.fit_transform(X_train_rand)\n",
        "X_val_rand_scaled = scaler_rand.transform(X_val_rand)\n",
        "X_test_rand_scaled = scaler_rand.transform(X_test_rand)\n",
        "\n",
        "y_train_rand_enc = le.transform(y_train_rand)\n",
        "y_val_rand_enc = le.transform(y_val_rand)\n",
        "y_test_rand_enc = le.transform(y_test_rand)\n",
        "\n",
        "# Temporal split preprocessing\n",
        "scaler_temp = StandardScaler()\n",
        "X_train_temp_scaled = scaler_temp.fit_transform(X_train_temp)\n",
        "X_val_temp_scaled = scaler_temp.transform(X_val_temp)\n",
        "X_test_temp_scaled = scaler_temp.transform(X_test_temp)\n",
        "\n",
        "y_train_temp_enc = le.transform(y_train_temp)\n",
        "y_val_temp_enc = le.transform(y_val_temp)\n",
        "y_test_temp_enc = le.transform(y_test_temp)\n",
        "\n",
        "print(\"Data preprocessed successfully!\")\n",
        "print(f\"\\nClass encoding: {dict(zip(CLASSES, le.transform(CLASSES)))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Models on Random Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine train and val for training (as done in original notebooks)\n",
        "X_train_rand_full = np.vstack([X_train_rand_scaled, X_val_rand_scaled])\n",
        "y_train_rand_full = np.concatenate([y_train_rand_enc, y_val_rand_enc])\n",
        "\n",
        "print(\"Training Logistic Regression on Random Split...\")\n",
        "lr_rand = LogisticRegression(\n",
        "    C=1e6,  # Minimal regularization\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced',\n",
        "    random_state=RANDOM_STATE,\n",
        "    solver='lbfgs',\n",
        "    multi_class='multinomial'\n",
        ")\n",
        "lr_rand.fit(X_train_rand_full, y_train_rand_full)\n",
        "\n",
        "print(\"Training kNN on Random Split...\")\n",
        "knn_rand = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    weights='distance',\n",
        "    n_jobs=-1\n",
        ")\n",
        "knn_rand.fit(X_train_rand_full, y_train_rand_full)\n",
        "\n",
        "print(\"Models trained on random split!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Models on Temporal Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine train and val for training\n",
        "X_train_temp_full = np.vstack([X_train_temp_scaled, X_val_temp_scaled])\n",
        "y_train_temp_full = np.concatenate([y_train_temp_enc, y_val_temp_enc])\n",
        "\n",
        "print(\"Training Logistic Regression on Temporal Split...\")\n",
        "lr_temp = LogisticRegression(\n",
        "    C=1e6,\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced',\n",
        "    random_state=RANDOM_STATE,\n",
        "    solver='lbfgs',\n",
        "    multi_class='multinomial'\n",
        ")\n",
        "lr_temp.fit(X_train_temp_full, y_train_temp_full)\n",
        "\n",
        "print(\"Training kNN on Temporal Split...\")\n",
        "knn_temp = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    weights='distance',\n",
        "    n_jobs=-1\n",
        ")\n",
        "knn_temp.fit(X_train_temp_full, y_train_temp_full)\n",
        "\n",
        "print(\"Models trained on temporal split!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluate Models and Compare Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model_simple(model, X_test, y_test, model_name, split_type, label_encoder):\n",
        "    \"\"\"Simplified evaluation function\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "    \n",
        "    # Per-class metrics\n",
        "    f1_per_class = f1_score(y_test, y_pred, average=None)\n",
        "    \n",
        "    results = {\n",
        "        'model_name': model_name,\n",
        "        'split_type': split_type,\n",
        "        'accuracy': float(accuracy),\n",
        "        'balanced_accuracy': float(balanced_acc),\n",
        "        'f1_macro': float(f1_macro),\n",
        "        'f1_weighted': float(f1_weighted),\n",
        "        'precision_macro': float(precision_macro),\n",
        "        'recall_macro': float(recall_macro),\n",
        "        'f1_per_class': {cls: float(f1_per_class[i]) for i, cls in enumerate(CLASSES)},\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Evaluate all models\n",
        "print(\"Evaluating models...\\n\")\n",
        "\n",
        "results_lr_rand = evaluate_model_simple(\n",
        "    lr_rand, X_test_rand_scaled, y_test_rand_enc,\n",
        "    'Logistic Regression', 'Random', le\n",
        ")\n",
        "\n",
        "results_knn_rand = evaluate_model_simple(\n",
        "    knn_rand, X_test_rand_scaled, y_test_rand_enc,\n",
        "    'kNN', 'Random', le\n",
        ")\n",
        "\n",
        "results_lr_temp = evaluate_model_simple(\n",
        "    lr_temp, X_test_temp_scaled, y_test_temp_enc,\n",
        "    'Logistic Regression', 'Temporal', le\n",
        ")\n",
        "\n",
        "results_knn_temp = evaluate_model_simple(\n",
        "    knn_temp, X_test_temp_scaled, y_test_temp_enc,\n",
        "    'kNN', 'Temporal', le\n",
        ")\n",
        "\n",
        "all_results = [results_lr_rand, results_knn_rand, results_lr_temp, results_knn_temp]\n",
        "\n",
        "# Print summary\n",
        "print(\"=\"*80)\n",
        "print(\"PERFORMANCE COMPARISON: RANDOM vs TEMPORAL SPLITS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n{'Model':<25} {'Split':<12} {'Accuracy':<12} {'Bal Acc':<12} {'F1 Macro':<12}\")\n",
        "print(\"-\"*80)\n",
        "for r in all_results:\n",
        "    print(f\"{r['model_name']:<25} {r['split_type']:<12} {r['accuracy']:<12.4f} {r['balanced_accuracy']:<12.4f} {r['f1_macro']:<12.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Accuracy comparison\n",
        "ax = axes[0, 0]\n",
        "models = ['Logistic Regression', 'kNN']\n",
        "random_accs = [results_lr_rand['accuracy'], results_knn_rand['accuracy']]\n",
        "temporal_accs = [results_lr_temp['accuracy'], results_knn_temp['accuracy']]\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "bars1 = ax.bar(x - width/2, random_accs, width, label='Random Split', color='steelblue', alpha=0.8)\n",
        "bars2 = ax.bar(x + width/2, temporal_accs, width, label='Temporal Split', color='coral', alpha=0.8)\n",
        "\n",
        "ax.set_ylabel('Accuracy', fontsize=12)\n",
        "ax.set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "               xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=9)\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "               xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 2. Balanced Accuracy comparison\n",
        "ax = axes[0, 1]\n",
        "random_ba = [results_lr_rand['balanced_accuracy'], results_knn_rand['balanced_accuracy']]\n",
        "temporal_ba = [results_lr_temp['balanced_accuracy'], results_knn_temp['balanced_accuracy']]\n",
        "\n",
        "bars3 = ax.bar(x - width/2, random_ba, width, label='Random Split', color='steelblue', alpha=0.8)\n",
        "bars4 = ax.bar(x + width/2, temporal_ba, width, label='Temporal Split', color='coral', alpha=0.8)\n",
        "\n",
        "ax.set_ylabel('Balanced Accuracy', fontsize=12)\n",
        "ax.set_title('Balanced Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "for bar in bars3:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "               xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=9)\n",
        "for bar in bars4:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "               xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 3. F1 Macro comparison\n",
        "ax = axes[1, 0]\n",
        "random_f1 = [results_lr_rand['f1_macro'], results_knn_rand['f1_macro']]\n",
        "temporal_f1 = [results_lr_temp['f1_macro'], results_knn_temp['f1_macro']]\n",
        "\n",
        "bars5 = ax.bar(x - width/2, random_f1, width, label='Random Split', color='steelblue', alpha=0.8)\n",
        "bars6 = ax.bar(x + width/2, temporal_f1, width, label='Temporal Split', color='coral', alpha=0.8)\n",
        "\n",
        "ax.set_ylabel('F1 Score (Macro)', fontsize=12)\n",
        "ax.set_title('F1 Macro Score Comparison', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "for bar in bars5:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "               xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=9)\n",
        "for bar in bars6:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "               xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 4. Performance difference\n",
        "ax = axes[1, 1]\n",
        "lr_diff = results_lr_rand['f1_macro'] - results_lr_temp['f1_macro']\n",
        "knn_diff = results_knn_rand['f1_macro'] - results_knn_temp['f1_macro']\n",
        "\n",
        "bars7 = ax.bar(models, [lr_diff, knn_diff], color=['#e74c3c' if d > 0 else '#27ae60' for d in [lr_diff, knn_diff]], alpha=0.8)\n",
        "ax.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "ax.set_ylabel('F1 Difference (Random - Temporal)', fontsize=12)\n",
        "ax.set_title('Performance Drop: Random → Temporal', fontsize=14, fontweight='bold')\n",
        "ax.set_xticklabels(models)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar in bars7:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:+.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "               xytext=(0, 3 if height > 0 else -15), textcoords='offset points', \n",
        "               ha='center', va='bottom' if height > 0 else 'top', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Per-Class Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-class F1 scores\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "for idx, (model_name, results_rand, results_temp) in enumerate([\n",
        "    ('Logistic Regression', results_lr_rand, results_lr_temp),\n",
        "    ('kNN', results_knn_rand, results_knn_temp)\n",
        "]):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    classes = CLASSES\n",
        "    random_f1 = [results_rand['f1_per_class'][cls] for cls in classes]\n",
        "    temporal_f1 = [results_temp['f1_per_class'][cls] for cls in classes]\n",
        "    \n",
        "    x = np.arange(len(classes))\n",
        "    width = 0.35\n",
        "    \n",
        "    bars1 = ax.bar(x - width/2, random_f1, width, label='Random Split', color='steelblue', alpha=0.8)\n",
        "    bars2 = ax.bar(x + width/2, temporal_f1, width, label='Temporal Split', color='coral', alpha=0.8)\n",
        "    \n",
        "    ax.set_ylabel('F1 Score', fontsize=12)\n",
        "    ax.set_title(f'{model_name}: Per-Class F1 Scores', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(classes)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    ax.set_ylim(0, 1)\n",
        "    \n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                   xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=9)\n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                   xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Confusion Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "# Logistic Regression - Random\n",
        "cm_lr_rand = np.array(results_lr_rand['confusion_matrix'])\n",
        "sns.heatmap(cm_lr_rand, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
        "            xticklabels=CLASSES, yticklabels=CLASSES)\n",
        "axes[0, 0].set_xlabel('Predicted', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Actual', fontsize=11)\n",
        "axes[0, 0].set_title('Logistic Regression - Random Split', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Logistic Regression - Temporal\n",
        "cm_lr_temp = np.array(results_lr_temp['confusion_matrix'])\n",
        "sns.heatmap(cm_lr_temp, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1],\n",
        "            xticklabels=CLASSES, yticklabels=CLASSES)\n",
        "axes[0, 1].set_xlabel('Predicted', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Actual', fontsize=11)\n",
        "axes[0, 1].set_title('Logistic Regression - Temporal Split', fontsize=12, fontweight='bold')\n",
        "\n",
        "# kNN - Random\n",
        "cm_knn_rand = np.array(results_knn_rand['confusion_matrix'])\n",
        "sns.heatmap(cm_knn_rand, annot=True, fmt='d', cmap='Greens', ax=axes[1, 0],\n",
        "            xticklabels=CLASSES, yticklabels=CLASSES)\n",
        "axes[1, 0].set_xlabel('Predicted', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Actual', fontsize=11)\n",
        "axes[1, 0].set_title('kNN - Random Split', fontsize=12, fontweight='bold')\n",
        "\n",
        "# kNN - Temporal\n",
        "cm_knn_temp = np.array(results_knn_temp['confusion_matrix'])\n",
        "sns.heatmap(cm_knn_temp, annot=True, fmt='d', cmap='Greens', ax=axes[1, 1],\n",
        "            xticklabels=CLASSES, yticklabels=CLASSES)\n",
        "axes[1, 1].set_xlabel('Predicted', fontsize=11)\n",
        "axes[1, 1].set_ylabel('Actual', fontsize=11)\n",
        "axes[1, 1].set_title('kNN - Temporal Split', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Detailed Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create detailed results table\n",
        "summary_data = []\n",
        "for r in all_results:\n",
        "    summary_data.append({\n",
        "        'Model': r['model_name'],\n",
        "        'Split Type': r['split_type'],\n",
        "        'Accuracy': f\"{r['accuracy']:.4f}\",\n",
        "        'Balanced Accuracy': f\"{r['balanced_accuracy']:.4f}\",\n",
        "        'F1 Macro': f\"{r['f1_macro']:.4f}\",\n",
        "        'F1 Weighted': f\"{r['f1_weighted']:.4f}\",\n",
        "        'Precision Macro': f\"{r['precision_macro']:.4f}\",\n",
        "        'Recall Macro': f\"{r['recall_macro']:.4f}\",\n",
        "        'F1 (Left)': f\"{r['f1_per_class']['left']:.4f}\",\n",
        "        'F1 (Forward)': f\"{r['f1_per_class']['forward']:.4f}\",\n",
        "        'F1 (Right)': f\"{r['f1_per_class']['right']:.4f}\",\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(summary_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Detailed Conclusion\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "Based on the experimental results comparing Random vs Temporal splits for Logistic Regression and kNN models:\n",
        "\n",
        "### Why Temporal Splits Show Lower Performance\n",
        "\n",
        "**1. Data Leakage in Random Splits**\n",
        "\n",
        "Random splits artificially inflate performance because:\n",
        "- **Consecutive frames are highly similar**: When driving on a track, frames captured milliseconds apart show nearly identical scenes (same road, same lighting, same obstacles)\n",
        "- **Random shuffling breaks temporal order**: A test frame might be from the same driving sequence as a training frame, allowing the model to \"memorize\" specific visual patterns\n",
        "- **The model sees similar patterns in both train and test**: This creates an unrealistic evaluation scenario where the model appears to generalize well, but is actually just recognizing patterns it has already seen in similar contexts\n",
        "\n",
        "**2. Temporal Correlation in Sequential Data**\n",
        "\n",
        "This dataset consists of consecutive frames from a car driving on a track:\n",
        "- Frames are captured in sequence as the car moves\n",
        "- Each frame is temporally correlated with its neighbors (similar to video frames)\n",
        "- The driving environment changes gradually over time (car position, road curvature, obstacles)\n",
        "- **Temporal splits respect this structure**: Training on earlier frames and testing on later frames simulates real-world deployment where the model must predict on future, unseen driving scenarios\n",
        "\n",
        "**3. Why Performance Drops with Temporal Splits**\n",
        "\n",
        "The performance drop observed when using temporal splits occurs because:\n",
        "\n",
        "**a) Distribution Shift**\n",
        "- The test set (later frames) may contain different driving scenarios than the training set (earlier frames)\n",
        "- Road conditions, lighting, or track sections might differ between training and test periods\n",
        "- The model must generalize to these new conditions, which is more challenging\n",
        "\n",
        "**b) True Generalization Test**\n",
        "- Temporal splits test whether the model learned generalizable features (e.g., \"left side brighter = turn left\") rather than memorizing specific frame sequences\n",
        "- Random splits allow the model to rely on spurious correlations from seeing similar frames in both train and test sets\n",
        "- The performance gap reveals how much the model was \"cheating\" by exploiting temporal similarity\n",
        "\n",
        "**c) Real-World Deployment Scenario**\n",
        "- In practice, an autonomous vehicle would use the model on future frames it hasn't seen\n",
        "- Temporal splits simulate this realistic scenario\n",
        "- The lower performance on temporal splits reflects the true difficulty of the task\n",
        "\n",
        "### Model-Specific Observations\n",
        "\n",
        "**Logistic Regression:**\n",
        "- Shows a performance drop when moving from random to temporal splits\n",
        "- This indicates that the linear model was benefiting from seeing similar frames in both train and test\n",
        "- The drop suggests that while logistic regression captures some generalizable patterns, it also relies on frame-to-frame similarities\n",
        "\n",
        "**k-Nearest Neighbors:**\n",
        "- Also shows a performance drop, potentially more pronounced than logistic regression\n",
        "- kNN is particularly sensitive to temporal correlation because it directly uses similarity to training examples\n",
        "- When test frames are temporally distant from training frames, kNN struggles more because it can't find similar neighbors in the training set\n",
        "- This highlights kNN's vulnerability to distribution shift\n",
        "\n",
        "### Implications for Model Selection\n",
        "\n",
        "1. **Temporal splits provide realistic evaluation**: Models should be evaluated on temporal splits to understand their true generalization capability\n",
        "\n",
        "2. **Random splits overestimate performance**: While useful for initial model development, random splits can give false confidence in model performance\n",
        "\n",
        "3. **Consider temporal models**: Given the sequential nature of this data, models that explicitly model temporal dependencies (e.g., LSTM, temporal CNNs) might perform better on temporal splits\n",
        "\n",
        "4. **Feature engineering matters**: The engineered features (spatial asymmetry, brightness centroids) should help with generalization, but the performance gap shows that temporal correlation still plays a significant role\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The performance difference between random and temporal splits demonstrates the critical importance of proper data splitting for sequential/temporal data. Random splits create an unrealistic evaluation scenario that inflates performance metrics, while temporal splits provide a more honest assessment of model generalization. For real-world deployment of steering prediction systems, models should be evaluated using temporal splits to ensure they can handle future, unseen driving scenarios.\n",
        "\n",
        "The observed performance drop is expected and actually desirable—it reveals the true difficulty of the task and prevents overconfidence in model capabilities. This analysis underscores why temporal splitting is essential for time-series and sequential data, even when the temporal structure might not be immediately obvious."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
